{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NB 01: Extract ENIGMA CORAL Data\n",
    "\n",
    "Extract geochemistry, community abundance, ASV taxonomy, and sample\n",
    "metadata from the ENIGMA CORAL database via Spark Connect. Filter to\n",
    "the 108 samples with both geochemistry and community data.\n",
    "\n",
    "**Requires Spark** — `get_spark_session()` from `berdl_notebook_utils`.\n",
    "\n",
    "**Outputs**: 4 TSV files in `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:38:27.899243Z",
     "iopub.status.busy": "2026-02-15T10:38:27.898954Z",
     "iopub.status.idle": "2026-02-15T10:38:28.367210Z",
     "shell.execute_reply": "2026-02-15T10:38:28.366204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from berdl_notebook_utils.setup_spark_session import get_spark_session\n",
    "spark = get_spark_session()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Extract Geochemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:38:28.370881Z",
     "iopub.status.busy": "2026-02-15T10:38:28.370392Z",
     "iopub.status.idle": "2026-02-15T10:38:29.242285Z",
     "shell.execute_reply": "2026-02-15T10:38:29.241448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw geochemistry: 52,884 rows\n",
      "  Samples: 117\n",
      "  Molecules: 48\n",
      "\n",
      "Geochemistry matrix: 117 samples x 48 molecules\n",
      "Key metals available: ['uranium', 'chromium', 'nickel', 'zinc', 'iron', 'copper', 'cadmium', 'lead', 'arsenic']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uranium</th>\n",
       "      <th>chromium</th>\n",
       "      <th>nickel</th>\n",
       "      <th>zinc</th>\n",
       "      <th>iron</th>\n",
       "      <th>copper</th>\n",
       "      <th>cadmium</th>\n",
       "      <th>lead</th>\n",
       "      <th>arsenic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>117.000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>117.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.442</td>\n",
       "      <td>0.225</td>\n",
       "      <td>4.756</td>\n",
       "      <td>0.730</td>\n",
       "      <td>13.682</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43.556</td>\n",
       "      <td>0.886</td>\n",
       "      <td>16.328</td>\n",
       "      <td>1.658</td>\n",
       "      <td>39.705</td>\n",
       "      <td>1.423</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.392</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.658</td>\n",
       "      <td>8.070</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>377.126</td>\n",
       "      <td>6.176</td>\n",
       "      <td>104.567</td>\n",
       "      <td>16.938</td>\n",
       "      <td>245.499</td>\n",
       "      <td>9.238</td>\n",
       "      <td>3.039</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uranium  chromium   nickel     zinc     iron   copper  cadmium  \\\n",
       "count  117.000   117.000  117.000  117.000  117.000  117.000  117.000   \n",
       "mean    10.442     0.225    4.756    0.730   13.682    0.368    0.107   \n",
       "std     43.556     0.886   16.328    1.658   39.705    1.423    0.391   \n",
       "min      0.000     0.000    0.001    0.076    0.017    0.001    0.000   \n",
       "25%      0.002     0.003    0.011    0.241    0.282    0.007    0.001   \n",
       "50%      0.005     0.009    0.038    0.426    1.392    0.027    0.002   \n",
       "75%      0.075     0.035    0.333    0.658    8.070    0.081    0.003   \n",
       "max    377.126     6.176  104.567   16.938  245.499    9.238    3.039   \n",
       "\n",
       "          lead  arsenic  \n",
       "count  117.000  117.000  \n",
       "mean     0.015    0.003  \n",
       "std      0.042    0.006  \n",
       "min      0.000    0.000  \n",
       "25%      0.001    0.000  \n",
       "50%      0.004    0.001  \n",
       "75%      0.011    0.003  \n",
       "max      0.311    0.061  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract geochemistry: sample x molecule concentrations\n",
    "geochem_raw = spark.sql(\"\"\"\n",
    "    SELECT sdt_sample_name, molecule_from_list_sys_oterm_name, concentration_micromolar\n",
    "    FROM enigma_coral.ddt_brick0000010\n",
    "\"\"\").toPandas()\n",
    "print(f\"Raw geochemistry: {len(geochem_raw):,} rows\")\n",
    "print(f\"  Samples: {geochem_raw['sdt_sample_name'].nunique()}\")\n",
    "print(f\"  Molecules: {geochem_raw['molecule_from_list_sys_oterm_name'].nunique()}\")\n",
    "\n",
    "# Pivot to sample x molecule matrix\n",
    "geochem = geochem_raw.pivot_table(\n",
    "    index='sdt_sample_name',\n",
    "    columns='molecule_from_list_sys_oterm_name',\n",
    "    values='concentration_micromolar',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Shorten column names\n",
    "geochem.columns = [c.replace(' atom', '').replace(' ', '_') for c in geochem.columns]\n",
    "print(f\"\\nGeochemistry matrix: {geochem.shape[0]} samples x {geochem.shape[1]} molecules\")\n",
    "\n",
    "# Focus on key contaminants\n",
    "key_metals = ['uranium', 'chromium', 'nickel', 'zinc', 'iron', 'copper', 'cadmium', 'lead', 'arsenic']\n",
    "available_metals = [m for m in key_metals if m in geochem.columns]\n",
    "print(f\"Key metals available: {available_metals}\")\n",
    "print()\n",
    "geochem[available_metals].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Extract Community Abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:38:29.244534Z",
     "iopub.status.busy": "2026-02-15T10:38:29.244415Z",
     "iopub.status.idle": "2026-02-15T10:38:34.609100Z",
     "shell.execute_reply": "2026-02-15T10:38:34.608334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap communities: 220\n",
      "  Samples: 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ASV counts (filtered to overlap): 135,504 rows\n",
      "  Communities: 212\n",
      "  Samples: 108\n",
      "  ASVs: 47528\n",
      "\n",
      "After aggregating replicates: 132,356 rows\n"
     ]
    }
   ],
   "source": [
    "# Extract ASV counts from brick476 (80M rows) -- filter in Spark first\n",
    "# brick476 is the table with 587 environmental communities and 108 geochemistry overlap\n",
    "# brick459 had no environmental community matches\n",
    "\n",
    "# Step 1: Get the community names for the 108 overlap samples\n",
    "# First get all geochemistry sample names\n",
    "geochem_sample_list = \"','\".join(geochem.index.tolist())\n",
    "\n",
    "overlap_communities = spark.sql(f\"\"\"\n",
    "    SELECT c.sdt_community_name, c.sdt_sample_name\n",
    "    FROM enigma_coral.sdt_community c\n",
    "    WHERE c.community_type_sys_oterm_name = 'Environmental Community'\n",
    "      AND c.sdt_sample_name IN ('{geochem_sample_list}')\n",
    "\"\"\").toPandas()\n",
    "print(f\"Overlap communities: {len(overlap_communities)}\")\n",
    "print(f\"  Samples: {overlap_communities['sdt_sample_name'].nunique()}\")\n",
    "\n",
    "# Step 2: Register as temp view for Spark join\n",
    "spark.createDataFrame(overlap_communities).createOrReplaceTempView(\"overlap_comms\")\n",
    "\n",
    "# Step 3: Extract ASV counts only for overlap communities (pushes filter to Spark)\n",
    "asv_counts_raw = spark.sql(\"\"\"\n",
    "    SELECT b.sdt_asv_name, b.sdt_community_name,\n",
    "           CAST(b.count_count_unit AS LONG) as read_count,\n",
    "           oc.sdt_sample_name\n",
    "    FROM enigma_coral.ddt_brick0000476 b\n",
    "    JOIN overlap_comms oc ON b.sdt_community_name = oc.sdt_community_name\n",
    "    WHERE CAST(b.count_count_unit AS LONG) > 0\n",
    "\"\"\").toPandas()\n",
    "print(f\"\\nASV counts (filtered to overlap): {len(asv_counts_raw):,} rows\")\n",
    "print(f\"  Communities: {asv_counts_raw['sdt_community_name'].nunique()}\")\n",
    "print(f\"  Samples: {asv_counts_raw['sdt_sample_name'].nunique()}\")\n",
    "print(f\"  ASVs: {asv_counts_raw['sdt_asv_name'].nunique()}\")\n",
    "\n",
    "# Aggregate replicates: sum counts per ASV per sample\n",
    "# brick476 has a replicate column -- aggregate across replicates\n",
    "asv_counts = asv_counts_raw.groupby(\n",
    "    ['sdt_asv_name', 'sdt_community_name', 'sdt_sample_name']\n",
    ")['read_count'].sum().reset_index()\n",
    "print(f\"\\nAfter aggregating replicates: {len(asv_counts):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Extract ASV Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:38:34.611169Z",
     "iopub.status.busy": "2026-02-15T10:38:34.611039Z",
     "iopub.status.idle": "2026-02-15T10:38:36.803723Z",
     "shell.execute_reply": "2026-02-15T10:38:36.802642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASV genus taxonomy: 96,822 rows\n",
      "  Unique ASVs: 96822\n",
      "  Unique genera: 1830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASV phylum taxonomy: 107,888 rows\n",
      "\n",
      "Combined taxonomy: 96,822 ASVs with genus + phylum\n"
     ]
    }
   ],
   "source": [
    "# Extract ASV -> genus mapping from brick454\n",
    "asv_taxonomy = spark.sql(\"\"\"\n",
    "    SELECT sdt_asv_name, taxonomic_level_sys_oterm_name as tax_level, sdt_taxon_name as taxon\n",
    "    FROM enigma_coral.ddt_brick0000454\n",
    "    WHERE taxonomic_level_sys_oterm_name = 'Genus'\n",
    "\"\"\").toPandas()\n",
    "print(f\"ASV genus taxonomy: {len(asv_taxonomy):,} rows\")\n",
    "print(f\"  Unique ASVs: {asv_taxonomy['sdt_asv_name'].nunique()}\")\n",
    "print(f\"  Unique genera: {asv_taxonomy['taxon'].nunique()}\")\n",
    "\n",
    "# Also get phylum-level for context\n",
    "asv_phylum = spark.sql(\"\"\"\n",
    "    SELECT sdt_asv_name, sdt_taxon_name as phylum\n",
    "    FROM enigma_coral.ddt_brick0000454\n",
    "    WHERE taxonomic_level_sys_oterm_name = 'Phylum'\n",
    "\"\"\").toPandas()\n",
    "print(f\"ASV phylum taxonomy: {len(asv_phylum):,} rows\")\n",
    "\n",
    "# Merge genus + phylum\n",
    "asv_tax_full = asv_taxonomy[['sdt_asv_name', 'taxon']].rename(columns={'taxon': 'genus'})\n",
    "asv_tax_full = asv_tax_full.merge(asv_phylum, on='sdt_asv_name', how='left')\n",
    "print(f\"\\nCombined taxonomy: {len(asv_tax_full):,} ASVs with genus + phylum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Build Sample Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:38:36.805904Z",
     "iopub.status.busy": "2026-02-15T10:38:36.805787Z",
     "iopub.status.idle": "2026-02-15T10:38:37.995236Z",
     "shell.execute_reply": "2026-02-15T10:38:37.994201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample metadata: 4,346 samples\n",
      "  Locations: 202\n"
     ]
    }
   ],
   "source": [
    "# Sample metadata: sample -> location -> date\n",
    "sample_meta = spark.sql(\"\"\"\n",
    "    SELECT s.sdt_sample_name, s.sdt_location_name, s.date, s.depth_meter,\n",
    "           l.latitude_degree, l.longitude_degree, l.region\n",
    "    FROM enigma_coral.sdt_sample s\n",
    "    LEFT JOIN enigma_coral.sdt_location l\n",
    "      ON s.sdt_location_name = l.sdt_location_name\n",
    "\"\"\").toPandas()\n",
    "print(f\"Sample metadata: {len(sample_meta):,} samples\")\n",
    "print(f\"  Locations: {sample_meta['sdt_location_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Find Overlapping Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:38:37.997553Z",
     "iopub.status.busy": "2026-02-15T10:38:37.997297Z",
     "iopub.status.idle": "2026-02-15T10:38:38.015290Z",
     "shell.execute_reply": "2026-02-15T10:38:38.014451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geochemistry samples: 117\n",
      "Community samples (from brick476 overlap): 108\n",
      "\n",
      "Filtered geochemistry: 108 samples\n",
      "Filtered ASV counts: 132,356 rows\n",
      "Filtered sample metadata: 108 samples\n"
     ]
    }
   ],
   "source": [
    "# Identify the overlap samples (already filtered in Spark extraction above)\n",
    "overlap_samples = set(asv_counts['sdt_sample_name'].unique())\n",
    "\n",
    "print(f\"Geochemistry samples: {len(geochem)}\") \n",
    "print(f\"Community samples (from brick476 overlap): {len(overlap_samples)}\")\n",
    "\n",
    "# Filter geochemistry to overlap\n",
    "geochem_filtered = geochem.loc[geochem.index.isin(overlap_samples)].copy()\n",
    "asv_counts_filtered = asv_counts.copy()  # already filtered\n",
    "sample_meta_filtered = sample_meta[sample_meta['sdt_sample_name'].isin(overlap_samples)].copy()\n",
    "\n",
    "print(f\"\\nFiltered geochemistry: {len(geochem_filtered)} samples\")\n",
    "print(f\"Filtered ASV counts: {len(asv_counts_filtered):,} rows\")\n",
    "print(f\"Filtered sample metadata: {len(sample_meta_filtered)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Save Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:38:38.018256Z",
     "iopub.status.busy": "2026-02-15T10:38:38.017995Z",
     "iopub.status.idle": "2026-02-15T10:38:38.307683Z",
     "shell.execute_reply": "2026-02-15T10:38:38.306817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: site_geochemistry.tsv (108 samples x 48 molecules)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: asv_counts.tsv (132,356 non-zero counts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: asv_taxonomy.tsv (96,822 ASVs)\n",
      "Saved: sample_metadata.tsv (108 samples)\n"
     ]
    }
   ],
   "source": [
    "# Save geochemistry\n",
    "geochem_filtered.to_csv(DATA_DIR / 'site_geochemistry.tsv', sep='\\t')\n",
    "print(f\"Saved: site_geochemistry.tsv ({len(geochem_filtered)} samples x {geochem_filtered.shape[1]} molecules)\")\n",
    "\n",
    "# Save ASV counts (only non-zero, filtered to overlap samples)\n",
    "asv_counts_filtered[['sdt_asv_name', 'sdt_community_name', 'sdt_sample_name', 'read_count']].to_csv(\n",
    "    DATA_DIR / 'asv_counts.tsv', sep='\\t', index=False\n",
    ")\n",
    "print(f\"Saved: asv_counts.tsv ({len(asv_counts_filtered):,} non-zero counts)\")\n",
    "\n",
    "# Save ASV taxonomy\n",
    "asv_tax_full.to_csv(DATA_DIR / 'asv_taxonomy.tsv', sep='\\t', index=False)\n",
    "print(f\"Saved: asv_taxonomy.tsv ({len(asv_tax_full):,} ASVs)\")\n",
    "\n",
    "# Save sample metadata\n",
    "sample_meta_filtered.to_csv(DATA_DIR / 'sample_metadata.tsv', sep='\\t', index=False)\n",
    "print(f\"Saved: sample_metadata.tsv ({len(sample_meta_filtered)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:38:38.310025Z",
     "iopub.status.busy": "2026-02-15T10:38:38.309905Z",
     "iopub.status.idle": "2026-02-15T10:38:38.322771Z",
     "shell.execute_reply": "2026-02-15T10:38:38.322028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NB01 SUMMARY: ENIGMA Data Extraction\n",
      "============================================================\n",
      "Overlap samples: 108\n",
      "Geochemistry: 48 molecules measured\n",
      "  Uranium range: 0.000 - 188.2 µM\n",
      "Community: 47,528 ASVs in 212 communities\n",
      "Taxonomy: 1,830 genera\n",
      "Locations: 97\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NB01 SUMMARY: ENIGMA Data Extraction\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Overlap samples: {len(overlap_samples)}\")\n",
    "print(f\"Geochemistry: {geochem_filtered.shape[1]} molecules measured\")\n",
    "print(f\"  Uranium range: {geochem_filtered.get('uranium', pd.Series([0])).min():.3f} - {geochem_filtered.get('uranium', pd.Series([0])).max():.1f} µM\")\n",
    "print(f\"Community: {asv_counts_filtered['sdt_asv_name'].nunique():,} ASVs in {asv_counts_filtered['sdt_community_name'].nunique()} communities\")\n",
    "print(f\"Taxonomy: {asv_tax_full['genus'].nunique():,} genera\")\n",
    "print(f\"Locations: {sample_meta_filtered['sdt_location_name'].nunique()}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
