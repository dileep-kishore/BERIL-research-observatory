---
reviewer: BERIL Automated Review
date: 2026-02-17
project: ecotype_env_reanalysis
---

# Review: Ecotype Reanalysis — Environmental-Only Samples

## Summary

This is an exemplary reanalysis project: tightly scoped, well-motivated by prior findings, and honest about a null result. The research question — whether clinical sampling bias explains the weak environment–gene content signal — is clearly derived from two parent projects. The statistical approach (Mann-Whitney U and continuous Spearman tests) is appropriate, and the notebook is well-organized with saved outputs and four high-quality visualizations. The REPORT.md provides unusually thoughtful interpretation of why the hypothesis was wrong, with multiple alternative explanations. The main issues are: (1) a factual discrepancy between the partial correlation ranges reported in REPORT.md and the actual notebook outputs; (2) the unresolved 27x discrepancy in median partial correlations versus the original analysis, which somewhat undermines confidence in the numerical findings; (3) stale "(TBD)" and "pending execution" notes in README.md and REPORT.md that contradict the project's completed status; and (4) one data file (`species_env_classification.csv`) is not generated by the notebook, creating a reproducibility gap.

## Methodology

**Research question**: Clearly stated and testable — "Does the environment effect on gene content become stronger when analysis is restricted to genuinely environmental samples?" The null and alternative hypotheses are well-formulated in RESEARCH_PLAN.md with explicit expected outcomes for each case.

**Approach**: Sound. Classifying species by majority genome-level `env_category` (from keyword-harmonized `isolation_source`) is a meaningful improvement over the original manual species-level categorization that had 56% of species as "Unknown." The Mann-Whitney U test with a one-sided alternative is appropriate for comparing medians between groups with different sample sizes and non-normal distributions. The supplementary continuous Spearman analysis (fraction environmental vs partial correlation) is a valuable robustness check that avoids the arbitrary 50% classification threshold.

**Data sources**: Clearly identified with a table in both the README and the notebook markdown header. The project reuses data from two parent projects with no new BERDL queries — a clean design that isolates the analytical question from data extraction concerns.

**Reproducibility**: Good overall. The README includes a Reproduction section with prerequisites and clear steps. There is a `requirements.txt` with pinned `kaleido==0.2.1` (correctly following the pitfall about kaleido v1 on headless pods). The notebook runs locally without Spark, which is a major advantage. However, reproduction depends on parent project data files existing at relative paths (`../../ecotype_analysis/data/` and `../../env_embedding_explorer/data/`) — this is documented but fragile if the directory structure changes. The `species_env_classification.csv` data file is not generated by the notebook (see Suggestions #4).

## Code Quality

**Notebook organization**: Excellent logical flow — setup → data loading → species classification → join with correlations → statistical tests → visualizations → NaN analysis → methodological comparison → summary → data export. Markdown cells provide clear section headers and explanatory text at each stage. The notebook reads as a narrative, not just code.

**Code correctness**: The classification logic (cells 7) correctly implements the majority-vote scheme with Environmental/Human-associated/Mixed categories. The statistical test (cell 12) correctly uses `alternative='greater'` for the one-sided hypothesis. The continuous Spearman analysis (cell 14) is a well-chosen complement to the binary test.

**Environment harmonization**: The `harmonize()` function in cell 5 re-implements the keyword-based categorization from the parent project rather than importing the pre-computed `env_category` column directly. This is both a strength (self-contained reproducibility) and a risk (potential divergence from the parent project's implementation). The notebook does import `isolation_source` from the parent's CSV and recompute categories, which means results depend on the harmonization logic being identical.

**NaN handling**: Well-addressed. Cell 10 explicitly quantifies NaN partial correlations by group (Environmental: 21%, Human-associated: 7%, Mixed/Other: 20%), and cell 21 identifies the largest NaN species. The differential NaN rates (environmental and mixed groups have 3x higher NaN rates than human-associated) are a potential confound that the report acknowledges.

**Pitfall awareness**: The project follows relevant pitfalls from `docs/pitfalls.md`:
- Correctly pins `kaleido==0.2.1` (env_embedding_explorer kaleido pitfall)
- Documents the Spark `maxResultSize` issue for K. pneumoniae
- The project itself contributed two pitfalls back to the repository (maxResultSize and broken symlinks), which is good practice
- Follows the three-file structure (README/RESEARCH_PLAN/REPORT) per the synthesize skill pitfall

**Minor code note**: In cell 7, `species_env['n_env'] = species_env[env_cols].sum(axis=1) if env_cols else 0` — the conditional operates on the truthiness of the list (empty = falsy), which works correctly but `if len(env_cols) > 0` would be more explicit.

## Findings Assessment

**Conclusions supported by data**: Yes. The null result (Mann-Whitney p=0.83, Spearman rho=-0.085, p=0.25) is clearly supported by the notebook outputs. The report correctly notes that the effect is in the *opposite* direction from the hypothesis — human-associated species have slightly higher partial correlations (median 0.084) than environmental species (median 0.051). The interpretation section provides three plausible explanations for this unexpected pattern.

**Factual discrepancy in REPORT.md**: The partial correlation ranges reported in REPORT.md do not match the notebook outputs. REPORT.md states Environmental range as [-0.16, 0.13], Human-associated as [-0.16, 0.19], and Mixed/Other as [-0.10, 0.20]. But the notebook output (cell 12) shows Environmental [-0.4971, 0.7822], Human-associated [-0.2971, 0.7259], Mixed/Other [-0.3811, 0.6935]. The medians and means are consistent, but the ranges are significantly different. This suggests the REPORT.md ranges may have been written from an earlier version of the analysis or a different dataset. This should be corrected.

**27x discrepancy**: The median partial correlation across all species is 0.081 in this reanalysis vs 0.003 in the original ecotype analysis. The report attributes this to methodological differences (no diversity-maximizing downsampling) and correctly notes it doesn't invalidate the group comparison. However, cell 23 only documents the discrepancy — it doesn't test whether the original (downsampled) correlations with the new genome-level classifications also show no group difference. This would be a valuable validation.

**Limitations acknowledged**: Thorough. The REPORT.md lists four specific limitations: no downsampling, NaN species exclusion, K. pneumoniae exclusion, and the majority-vote threshold. The Future Directions section proposes concrete follow-up analyses.

**Stale status markers**: The README (line 26) says "(TBD)" for the report link, but the report is complete. The REPORT.md (line 123) says the notebook is "pending execution with saved outputs," but the notebook has been executed with outputs saved. These should be updated.

**Visualizations**: Four figures are saved in both PNG and interactive HTML formats, covering: (1) box plot of partial correlations by group, (2) overlaid histograms, (3) scatter of continuous fraction environmental vs partial correlation, and (4) pie chart of species classification. All are properly labeled with titles, axis labels, and reference lines. The box plot clearly shows the overlapping distributions that support the null finding.

## Suggestions

1. **Important — Fix REPORT.md partial correlation ranges**: The ranges in the Results section (e.g., Environmental "[-0.16, 0.13]") do not match the notebook output ([-0.4971, 0.7822]). Update REPORT.md to reflect the actual data. The medians and means appear correct; only the range values are wrong.

2. **Important — Update stale status markers**: (a) README.md line 26: change `(TBD)` to remove the TBD since the report is complete. (b) REPORT.md line 123: change "pending execution with saved outputs" to indicate the notebook has been executed, since it has saved outputs in 15 of 16 code cells.

3. **Important — Investigate the 27x partial correlation discrepancy**: The report flags this as the first Future Direction but doesn't attempt even a simple validation. If the original `ecotype_analysis` data contains the 172-species downsampled results, re-running the Mann-Whitney U test on those values with the new genome-level classifications would confirm the null result is robust to the methodological difference. This could be added as a few cells at the end of the notebook.

4. **Moderate — Generate `species_env_classification.csv` from the notebook**: The `data/species_env_classification.csv` file (224 rows) exists but is not produced by the notebook. The notebook saves `ecotype_corr_with_env_group.csv` (cell 26) but never calls `species_env.to_csv(...)` for the classification file. Adding this save step would close the reproducibility gap — currently a reader running the notebook would not regenerate this file.

5. **Minor — Consider effect size reporting**: The Mann-Whitney U test gives p=0.83, but reporting the rank-biserial correlation (effect size) would quantify how trivially small the group difference is. With `scipy.stats.mannwhitneyu`, the rank-biserial r = 1 - 2U/(n₁×n₂) can be computed from the existing U statistic.

6. **Minor — Document the re-harmonization decision**: Cell 5 re-implements the keyword-based `harmonize()` function rather than importing the pre-computed `env_category` from the parent project's CSV (which already has that column). This is fine for reproducibility, but a brief markdown note explaining why (e.g., "re-implemented to be self-contained") would help readers understand the choice and verify consistency with the parent project.

## Review Metadata
- **Reviewer**: BERIL Automated Review
- **Date**: 2026-02-17
- **Scope**: README.md, RESEARCH_PLAN.md, REPORT.md, references.md, requirements.txt, 1 notebook (16 code cells, 15 with saved outputs), 2 data files, 4 figures (8 files: PNG + HTML)
- **Note**: This review was generated by an AI system. It should be treated as advisory input, not a definitive assessment.
