{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB01: Data Extraction — AlphaEarth Embeddings + Environment Labels\n",
    "\n",
    "**Requires BERDL JupyterHub** — `get_spark_session()` is only available in JupyterHub kernels.\n",
    "\n",
    "## Background\n",
    "\n",
    "The BERDL pangenome database includes **AlphaEarth environmental embeddings** — 64-dimensional vectors derived from satellite imagery at each genome's sampling location. These embeddings encode environmental context (climate, land use, vegetation, etc.) but their relationship to traditional environment metadata has not been systematically characterized.\n",
    "\n",
    "This notebook extracts and joins three data layers for downstream interactive exploration:\n",
    "\n",
    "1. **AlphaEarth embeddings** (`alphaearth_embeddings_all_years`) — 83K genomes with 64-dim vectors, cleaned lat/lon, and taxonomy\n",
    "2. **NCBI environment metadata** (`ncbi_env`) — free-text environment labels in Entity-Attribute-Value format\n",
    "3. **Coverage statistics** — which genomes have which metadata fields populated\n",
    "\n",
    "### Key considerations\n",
    "\n",
    "- AlphaEarth coverage is only **28.4%** of all 293K genomes — biased toward genomes with valid lat/lon metadata\n",
    "- `ncbi_env` is an EAV table (multiple rows per genome) — we pivot it into one row per genome\n",
    "- The `isolation_source` field is free text with thousands of unique values — harmonization happens in NB02\n",
    "\n",
    "### Outputs\n",
    "\n",
    "All saved to `../data/`:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `alphaearth_with_env.csv` | Merged embeddings + pivoted env labels (one row per genome) |\n",
    "| `coverage_stats.csv` | Per-attribute population rates |\n",
    "| `ncbi_env_attribute_counts.csv` | Full inventory of all 334 harmonized_name values |\n",
    "| `isolation_source_raw_counts.csv` | Raw value frequencies for harmonization in NB02 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# get_spark_session() is injected into JupyterHub kernels — no import needed\n",
    "spark = get_spark_session()\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print('Spark session initialized')\n",
    "print(f'Output directory: {os.path.abspath(DATA_DIR)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract AlphaEarth Embeddings\n",
    "\n",
    "The `alphaearth_embeddings_all_years` table contains one row per genome with:\n",
    "- **64 embedding dimensions** (A00–A63): satellite-derived environmental vectors, values in [-0.54, 0.54]\n",
    "- **Cleaned coordinates**: `cleaned_lat`, `cleaned_lon` — parsed and validated from NCBI metadata\n",
    "- **Taxonomy**: full GTDB hierarchy from domain to species\n",
    "- **Biosample links**: `ncbi_biosample_accession_id` for joining to `ncbi_env`\n",
    "\n",
    "At 83K rows, this is small enough to collect entirely to the driver node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM kbase_ke_pangenome.alphaearth_embeddings_all_years\n",
    "\"\"\").toPandas()\n",
    "\n",
    "emb_cols = [c for c in ae_df.columns if c.startswith('A') and c[1:].isdigit()]\n",
    "\n",
    "print(f'AlphaEarth embeddings: {len(ae_df):,} genomes')\n",
    "print(f'Embedding dimensions: {len(emb_cols)}')\n",
    "print(f'Lat/lon non-null: {ae_df[\"cleaned_lat\"].notna().sum():,} / {len(ae_df):,}')\n",
    "print(f'Year range: {ae_df[\"cleaned_year\"].min()} – {ae_df[\"cleaned_year\"].max()}')\n",
    "print(f'\\nTaxonomy coverage:')\n",
    "for col in ['domain', 'phylum', 'class', 'order', 'family', 'genus', 'species']:\n",
    "    if col in ae_df.columns:\n",
    "        print(f'  {col}: {ae_df[col].nunique():,} unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inventory NCBI Environment Attributes\n",
    "\n",
    "The `ncbi_env` table uses an **Entity-Attribute-Value (EAV)** format — each row is one attribute for one biosample. Before we pivot, let's see what attributes exist and how many genomes have each.\n",
    "\n",
    "This inventory helps us decide which attributes to pivot into columns (we want the most-populated ones) and reveals the overall metadata landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_counts = spark.sql(\"\"\"\n",
    "    SELECT harmonized_name,\n",
    "           COUNT(*) as n_rows,\n",
    "           COUNT(DISTINCT accession) as n_genomes\n",
    "    FROM kbase_ke_pangenome.ncbi_env\n",
    "    GROUP BY harmonized_name\n",
    "    ORDER BY n_genomes DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'Total distinct harmonized_name values: {len(attr_counts)}')\n",
    "print(f'\\nTop 30 attributes by number of genomes:')\n",
    "attr_counts.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most-populated attributes are `collection_date` (273K genomes), `geo_loc_name` (272K), and `isolation_source` (245K). The ENVO ontology fields (`env_broad_scale`, `env_local_scale`, `env_medium`) cover ~80-88K genomes each — roughly matching the AlphaEarth coverage, which makes sense since both require geographic metadata.\n",
    "\n",
    "We'll pivot the most useful attributes into columns for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_counts.to_csv(os.path.join(DATA_DIR, 'ncbi_env_attribute_counts.csv'), index=False)\n",
    "print(f'Saved ncbi_env_attribute_counts.csv ({len(attr_counts)} attributes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pivot NCBI Environment Labels for AlphaEarth Genomes\n",
    "\n",
    "We join `ncbi_env` to our AlphaEarth genomes via `ncbi_biosample_accession_id` and pivot selected attributes from rows into columns.\n",
    "\n",
    "The approach:\n",
    "1. Register AlphaEarth biosample IDs as a Spark temp view for efficient joining\n",
    "2. Use `MAX(CASE WHEN ...)` to pivot each attribute into its own column\n",
    "3. This gives us one row per genome with all environment fields as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register AlphaEarth biosample IDs as temp view for Spark join\n",
    "biosample_ids = ae_df['ncbi_biosample_accession_id'].dropna().unique().tolist()\n",
    "print(f'AlphaEarth genomes with biosample IDs: {len(biosample_ids):,}')\n",
    "\n",
    "biosample_sdf = spark.createDataFrame(\n",
    "    [(b,) for b in biosample_ids],\n",
    "    ['accession']\n",
    ")\n",
    "biosample_sdf.createOrReplaceTempView('ae_biosamples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot key environment attributes into columns\n",
    "env_pivot = spark.sql(\"\"\"\n",
    "    SELECT ne.accession,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'isolation_source' THEN ne.content END) as isolation_source,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'geo_loc_name' THEN ne.content END) as geo_loc_name,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'env_broad_scale' THEN ne.content END) as env_broad_scale,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'env_local_scale' THEN ne.content END) as env_local_scale,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'env_medium' THEN ne.content END) as env_medium,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'host' THEN ne.content END) as host,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'collection_date' THEN ne.content END) as collection_date,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'lat_lon' THEN ne.content END) as lat_lon,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'depth' THEN ne.content END) as depth,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'altitude' THEN ne.content END) as altitude,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'temp' THEN ne.content END) as temp\n",
    "    FROM kbase_ke_pangenome.ncbi_env ne\n",
    "    JOIN ae_biosamples ab ON ne.accession = ab.accession\n",
    "    GROUP BY ne.accession\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'Environment labels pivoted for {len(env_pivot):,} genomes')\n",
    "print(f'\\nAttribute population rates:')\n",
    "for col in env_pivot.columns[1:]:\n",
    "    n = env_pivot[col].notna().sum()\n",
    "    pct = 100 * n / len(env_pivot) if len(env_pivot) > 0 else 0\n",
    "    print(f'  {col}: {n:,} ({pct:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all AlphaEarth genomes have `geo_loc_name` and `collection_date` (100%), and 92% have `isolation_source`. The ENVO ontology fields (`env_broad_scale`, `env_local_scale`, `env_medium`) cover about 38-42% — these may be cleaner than free-text `isolation_source` but have lower coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Embeddings with Environment Labels\n",
    "\n",
    "Join on `ncbi_biosample_accession_id` (left join to keep all AlphaEarth genomes, even those without env labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = ae_df.merge(\n",
    "    env_pivot,\n",
    "    left_on='ncbi_biosample_accession_id',\n",
    "    right_on='accession',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "if 'accession' in merged.columns:\n",
    "    merged = merged.drop(columns=['accession'])\n",
    "\n",
    "print(f'Merged dataset: {len(merged):,} genomes')\n",
    "print(f'  With isolation_source: {merged[\"isolation_source\"].notna().sum():,}')\n",
    "print(f'  Without: {merged[\"isolation_source\"].isna().sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Coverage Statistics\n",
    "\n",
    "Compute per-attribute population rates and boolean flags. These flags will be used in NB02 for UpSet-style intersection plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_cols = {\n",
    "    'has_latlon': merged['cleaned_lat'].notna() & merged['cleaned_lon'].notna(),\n",
    "    'has_isolation_source': merged['isolation_source'].notna(),\n",
    "    'has_env_broad_scale': merged['env_broad_scale'].notna(),\n",
    "    'has_env_local_scale': merged['env_local_scale'].notna(),\n",
    "    'has_env_medium': merged['env_medium'].notna(),\n",
    "    'has_host': merged['host'].notna(),\n",
    "    'has_geo_loc_name': merged['geo_loc_name'].notna(),\n",
    "}\n",
    "\n",
    "for name, flag in flag_cols.items():\n",
    "    merged[name] = flag\n",
    "\n",
    "coverage = pd.DataFrame([\n",
    "    {'attribute': name.replace('has_', ''), 'n_genomes': int(flag.sum()),\n",
    "     'pct_of_alphaearth': round(100 * flag.mean(), 1)}\n",
    "    for name, flag in flag_cols.items()\n",
    "])\n",
    "\n",
    "print('Coverage of AlphaEarth genomes:')\n",
    "coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage.to_csv(os.path.join(DATA_DIR, 'coverage_stats.csv'), index=False)\n",
    "print('Saved coverage_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Isolation Source Value Counts\n",
    "\n",
    "The `isolation_source` field is free text entered by submitters — thousands of unique values that need harmonization. We save the raw value counts so NB02 can build a keyword-based mapping to broad categories (Soil, Marine, Human gut, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_counts = (\n",
    "    merged.loc[merged['isolation_source'].notna(), 'isolation_source']\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    ")\n",
    "iso_counts.columns = ['isolation_source', 'count']\n",
    "\n",
    "print(f'Unique isolation_source values: {len(iso_counts):,}')\n",
    "print(f'\\nTop 30 values:')\n",
    "iso_counts.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top values are dominated by clinical samples (feces, blood, sputum, stool, urine), reflecting the strong bias toward pathogen genomes in NCBI. Environmental samples (soil, seawater, groundwater) are present but less common. The `missing` and `Unknown` values will need to be grouped with the unclassified category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_counts.to_csv(os.path.join(DATA_DIR, 'isolation_source_raw_counts.csv'), index=False)\n",
    "print('Saved isolation_source_raw_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(DATA_DIR, 'alphaearth_with_env.csv')\n",
    "merged.to_csv(out_path, index=False)\n",
    "\n",
    "print(f'Saved alphaearth_with_env.csv')\n",
    "print(f'  Rows: {len(merged):,}')\n",
    "print(f'  Columns: {len(merged.columns)}')\n",
    "print(f'  File size: {os.path.getsize(out_path) / 1e6:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sanity Checks\n",
    "\n",
    "Quick validation that the data looks reasonable before passing to NB02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_cols = [f'A{i:02d}' for i in range(64)]\n",
    "emb_stats = merged[emb_cols].describe().T\n",
    "\n",
    "print('Embedding dimensions (A00–A63):')\n",
    "print(f'  Value range: [{emb_stats[\"min\"].min():.3f}, {emb_stats[\"max\"].max():.3f}]')\n",
    "print(f'  Mean of means: {emb_stats[\"mean\"].mean():.3f}')\n",
    "print(f'  Mean of stds:  {emb_stats[\"std\"].mean():.3f}')\n",
    "print(f'  Any NaN: {merged[emb_cols].isna().any().any()}')\n",
    "print(f'  Genomes with any NaN embedding: {merged[emb_cols].isna().any(axis=1).sum():,}')\n",
    "\n",
    "print(f'\\nGeographic extent:')\n",
    "print(f'  Latitude:  [{merged[\"cleaned_lat\"].min():.2f}, {merged[\"cleaned_lat\"].max():.2f}]')\n",
    "print(f'  Longitude: [{merged[\"cleaned_lon\"].min():.2f}, {merged[\"cleaned_lon\"].max():.2f}]')\n",
    "\n",
    "print(f'\\nTop 10 phyla (of {merged[\"phylum\"].nunique()} total):')\n",
    "merged['phylum'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We extracted **83,287 genomes** with 64-dimensional AlphaEarth environmental embeddings and joined them with NCBI environment metadata. Key observations:\n",
    "\n",
    "- **Nearly all** genomes have lat/lon coordinates (99.99%) and geographic location names (100%)\n",
    "- **91.6%** have an `isolation_source` label (5,774 unique raw values — needs harmonization)\n",
    "- **38–42%** have structured ENVO ontology terms (`env_broad_scale`, `env_local_scale`, `env_medium`)\n",
    "- **63.7%** have a `host` field — reflecting the clinical sample bias\n",
    "- Some genomes (~3,838) have NaN in embedding dimensions — these will be filtered in NB02\n",
    "\n",
    "Next: `02_interactive_exploration.ipynb` for coordinate QC, environment harmonization, UMAP visualization, and geographic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Data extraction complete ===')\n",
    "print(f'\\nOutput files in {os.path.abspath(DATA_DIR)}:')\n",
    "for f in sorted(os.listdir(DATA_DIR)):\n",
    "    if f.endswith('.csv'):\n",
    "        size = os.path.getsize(os.path.join(DATA_DIR, f)) / 1e6\n",
    "        print(f'  {f} ({size:.1f} MB)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
