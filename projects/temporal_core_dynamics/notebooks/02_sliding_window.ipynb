{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Temporal Core Genome Dynamics - Sliding Window Analysis\n",
    "\n",
    "## Goal\n",
    "Calculate how the core genome changes as we progressively add genomes sorted by collection date.\n",
    "\n",
    "## Approaches\n",
    "1. **Cumulative Expansion**: Start with earliest 30 genomes, add one at a time, track core\n",
    "2. **Fixed Time Windows**: Group into 2-year bins, calculate core within each bin\n",
    "\n",
    "## Core Thresholds\n",
    "- 90% presence\n",
    "- 95% presence  \n",
    "- 99% presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and load data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = \"/home/psdehal/pangenome_science/data/temporal_core\"\n",
    "OUTPUT_PATH = DATA_PATH  # Same directory\n",
    "\n",
    "SPECIES = ['p_aeruginosa', 'a_baumannii']\n",
    "THRESHOLDS = [0.90, 0.95, 0.99]\n",
    "MIN_WINDOW_SIZE = 30  # Minimum genomes for core calculation\n",
    "\n",
    "print(f\"Loading data from: {DATA_PATH}\")\n",
    "print(f\"Thresholds: {THRESHOLDS}\")\n",
    "print(f\"Minimum window size: {MIN_WINDOW_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load genome and cluster data\n",
    "\n",
    "data = {}\n",
    "\n",
    "for species in SPECIES:\n",
    "    print(f\"\\n=== Loading {species} ===\")\n",
    "    \n",
    "    # Load genomes with dates\n",
    "    genomes_df = pd.read_parquet(f\"{DATA_PATH}/{species}_genomes.parquet\")\n",
    "    print(f\"  Genomes: {len(genomes_df)}\")\n",
    "    \n",
    "    # Load gene clusters\n",
    "    clusters_path = f\"{DATA_PATH}/{species}_gene_clusters\"\n",
    "    if os.path.exists(f\"{clusters_path}/all_clusters.parquet\"):\n",
    "        clusters_df = pd.read_parquet(f\"{clusters_path}/all_clusters.parquet\")\n",
    "    else:\n",
    "        chunks = []\n",
    "        for f in sorted(os.listdir(clusters_path)):\n",
    "            if f.endswith('.parquet'):\n",
    "                chunks.append(pd.read_parquet(f\"{clusters_path}/{f}\"))\n",
    "        clusters_df = pd.concat(chunks, ignore_index=True)\n",
    "    print(f\"  Cluster memberships: {len(clusters_df):,}\")\n",
    "    \n",
    "    # Create genome-to-clusters mapping (set of clusters per genome)\n",
    "    genome_clusters = clusters_df.groupby('genome_id')['gene_cluster_id'].apply(set).to_dict()\n",
    "    print(f\"  Unique clusters: {clusters_df['gene_cluster_id'].nunique():,}\")\n",
    "    \n",
    "    # Get ordered list of genomes by collection date\n",
    "    genomes_df = genomes_df.sort_values('collection_date').reset_index(drop=True)\n",
    "    ordered_genomes = genomes_df['genome_id'].tolist()\n",
    "    \n",
    "    data[species] = {\n",
    "        'genomes_df': genomes_df,\n",
    "        'genome_clusters': genome_clusters,\n",
    "        'ordered_genomes': ordered_genomes,\n",
    "        'all_clusters': set(clusters_df['gene_cluster_id'].unique())\n",
    "    }\n",
    "    \n",
    "    print(f\"  Date range: {genomes_df['collection_date'].min()} to {genomes_df['collection_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Core Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define core calculation functions\n",
    "\n",
    "def calculate_core(genome_ids, genome_clusters, threshold):\n",
    "    \"\"\"\n",
    "    Calculate core genome for a set of genomes at a given threshold.\n",
    "    \n",
    "    Args:\n",
    "        genome_ids: List of genome IDs to include\n",
    "        genome_clusters: Dict mapping genome_id -> set of cluster_ids\n",
    "        threshold: Fraction of genomes that must have a cluster (e.g., 0.95)\n",
    "    \n",
    "    Returns:\n",
    "        set of cluster_ids that are 'core' (present in >= threshold fraction)\n",
    "    \"\"\"\n",
    "    n_genomes = len(genome_ids)\n",
    "    min_presence = int(np.ceil(threshold * n_genomes))\n",
    "    \n",
    "    # Count presence of each cluster\n",
    "    cluster_counts = defaultdict(int)\n",
    "    for gid in genome_ids:\n",
    "        if gid in genome_clusters:\n",
    "            for cid in genome_clusters[gid]:\n",
    "                cluster_counts[cid] += 1\n",
    "    \n",
    "    # Filter to core\n",
    "    core = {cid for cid, count in cluster_counts.items() if count >= min_presence}\n",
    "    \n",
    "    return core\n",
    "\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets.\"\"\"\n",
    "    if len(set1) == 0 and len(set2) == 0:\n",
    "        return 1.0\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "# Test on a small subset\n",
    "test_species = SPECIES[0]\n",
    "test_genomes = data[test_species]['ordered_genomes'][:50]\n",
    "test_core = calculate_core(test_genomes, data[test_species]['genome_clusters'], 0.95)\n",
    "print(f\"Test: Core size for first 50 {test_species} genomes at 95%: {len(test_core)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Approach 1: Cumulative Expansion\n",
    "\n",
    "Sort genomes by collection date, then progressively add genomes and track core size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Cumulative expansion analysis\n",
    "\n",
    "cumulative_results = []\n",
    "\n",
    "for species in SPECIES:\n",
    "    print(f\"\\n=== Cumulative analysis: {species} ===\")\n",
    "    \n",
    "    ordered_genomes = data[species]['ordered_genomes']\n",
    "    genome_clusters = data[species]['genome_clusters']\n",
    "    genomes_df = data[species]['genomes_df']\n",
    "    \n",
    "    n_total = len(ordered_genomes)\n",
    "    \n",
    "    # Sample points (every 10 genomes to speed up, plus key points)\n",
    "    sample_points = list(range(MIN_WINDOW_SIZE, n_total, 10))\n",
    "    if n_total not in sample_points:\n",
    "        sample_points.append(n_total)\n",
    "    \n",
    "    print(f\"  Calculating core at {len(sample_points)} sample points...\")\n",
    "    \n",
    "    prev_cores = {t: None for t in THRESHOLDS}\n",
    "    \n",
    "    for n in tqdm(sample_points):\n",
    "        current_genomes = ordered_genomes[:n]\n",
    "        \n",
    "        # Get date of last genome added\n",
    "        last_date = genomes_df.iloc[n-1]['collection_date']\n",
    "        \n",
    "        for threshold in THRESHOLDS:\n",
    "            core = calculate_core(current_genomes, genome_clusters, threshold)\n",
    "            \n",
    "            # Calculate change from previous\n",
    "            if prev_cores[threshold] is not None:\n",
    "                jaccard = jaccard_similarity(core, prev_cores[threshold])\n",
    "                gained = len(core - prev_cores[threshold])\n",
    "                lost = len(prev_cores[threshold] - core)\n",
    "            else:\n",
    "                jaccard = None\n",
    "                gained = None\n",
    "                lost = None\n",
    "            \n",
    "            cumulative_results.append({\n",
    "                'species': species,\n",
    "                'approach': 'cumulative',\n",
    "                'n_genomes': n,\n",
    "                'last_date': last_date,\n",
    "                'threshold': threshold,\n",
    "                'core_size': len(core),\n",
    "                'jaccard_vs_prev': jaccard,\n",
    "                'gained': gained,\n",
    "                'lost': lost\n",
    "            })\n",
    "            \n",
    "            prev_cores[threshold] = core\n",
    "    \n",
    "    # Summary\n",
    "    for threshold in THRESHOLDS:\n",
    "        species_results = [r for r in cumulative_results \n",
    "                          if r['species'] == species and r['threshold'] == threshold]\n",
    "        initial = species_results[0]['core_size']\n",
    "        final = species_results[-1]['core_size']\n",
    "        print(f\"  {int(threshold*100)}% threshold: {initial} -> {final} ({100*(final-initial)/initial:+.1f}%)\")\n",
    "\n",
    "cumulative_df = pd.DataFrame(cumulative_results)\n",
    "print(f\"\\nTotal results: {len(cumulative_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Visualize cumulative core decay\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {0.90: 'blue', 0.95: 'green', 0.99: 'red'}\n",
    "\n",
    "for idx, species in enumerate(SPECIES):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    species_data = cumulative_df[cumulative_df['species'] == species]\n",
    "    \n",
    "    for threshold in THRESHOLDS:\n",
    "        thresh_data = species_data[species_data['threshold'] == threshold]\n",
    "        ax.plot(thresh_data['n_genomes'], thresh_data['core_size'], \n",
    "                color=colors[threshold], label=f\"{int(threshold*100)}% threshold\",\n",
    "                linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Number of Genomes (chronological)')\n",
    "    ax.set_ylabel('Core Size (gene clusters)')\n",
    "    ax.set_title(f'{species}\\nCore Decay by Collection Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/cumulative_core_decay.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: cumulative_core_decay.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## Approach 2: Fixed Time Windows\n",
    "\n",
    "Group genomes into 2-year bins, calculate core within each bin independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Fixed time window analysis\n",
    "\n",
    "WINDOW_YEARS = 2  # 2-year windows\n",
    "\n",
    "window_results = []\n",
    "window_cores = {}  # Store actual core sets for comparison\n",
    "\n",
    "for species in SPECIES:\n",
    "    print(f\"\\n=== Time window analysis: {species} ===\")\n",
    "    \n",
    "    genomes_df = data[species]['genomes_df'].copy()\n",
    "    genome_clusters = data[species]['genome_clusters']\n",
    "    \n",
    "    # Create year bins\n",
    "    genomes_df['year'] = genomes_df['collection_date'].dt.year\n",
    "    min_year = genomes_df['year'].min()\n",
    "    max_year = genomes_df['year'].max()\n",
    "    \n",
    "    # Create 2-year windows\n",
    "    bins = list(range(min_year, max_year + WINDOW_YEARS, WINDOW_YEARS))\n",
    "    genomes_df['window'] = pd.cut(genomes_df['year'], bins=bins, right=False,\n",
    "                                   labels=[f\"{bins[i]}-{bins[i+1]-1}\" for i in range(len(bins)-1)])\n",
    "    \n",
    "    print(f\"  Year range: {min_year} - {max_year}\")\n",
    "    print(f\"  Windows: {len(bins)-1}\")\n",
    "    \n",
    "    window_cores[species] = {}\n",
    "    \n",
    "    for window_label in genomes_df['window'].cat.categories:\n",
    "        window_genomes = genomes_df[genomes_df['window'] == window_label]['genome_id'].tolist()\n",
    "        n_genomes = len(window_genomes)\n",
    "        \n",
    "        if n_genomes < MIN_WINDOW_SIZE:\n",
    "            print(f\"    {window_label}: {n_genomes} genomes (< {MIN_WINDOW_SIZE}, skipping)\")\n",
    "            continue\n",
    "        \n",
    "        window_cores[species][window_label] = {}\n",
    "        \n",
    "        for threshold in THRESHOLDS:\n",
    "            core = calculate_core(window_genomes, genome_clusters, threshold)\n",
    "            \n",
    "            window_results.append({\n",
    "                'species': species,\n",
    "                'approach': 'fixed_window',\n",
    "                'window': window_label,\n",
    "                'n_genomes': n_genomes,\n",
    "                'threshold': threshold,\n",
    "                'core_size': len(core)\n",
    "            })\n",
    "            \n",
    "            window_cores[species][window_label][threshold] = core\n",
    "        \n",
    "        print(f\"    {window_label}: {n_genomes} genomes, 95% core = {len(window_cores[species][window_label][0.95])}\")\n",
    "\n",
    "window_df = pd.DataFrame(window_results)\n",
    "print(f\"\\nTotal window results: {len(window_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Calculate inter-window Jaccard similarities\n",
    "\n",
    "# Compare core composition between adjacent time windows\n",
    "jaccard_results = []\n",
    "\n",
    "for species in SPECIES:\n",
    "    print(f\"\\n=== Inter-window similarity: {species} ===\")\n",
    "    \n",
    "    windows = sorted(window_cores[species].keys())\n",
    "    \n",
    "    for threshold in THRESHOLDS:\n",
    "        print(f\"  {int(threshold*100)}% threshold:\")\n",
    "        \n",
    "        for i in range(len(windows) - 1):\n",
    "            w1, w2 = windows[i], windows[i+1]\n",
    "            core1 = window_cores[species][w1].get(threshold, set())\n",
    "            core2 = window_cores[species][w2].get(threshold, set())\n",
    "            \n",
    "            if len(core1) == 0 or len(core2) == 0:\n",
    "                continue\n",
    "            \n",
    "            jaccard = jaccard_similarity(core1, core2)\n",
    "            shared = len(core1 & core2)\n",
    "            only_w1 = len(core1 - core2)\n",
    "            only_w2 = len(core2 - core1)\n",
    "            \n",
    "            jaccard_results.append({\n",
    "                'species': species,\n",
    "                'threshold': threshold,\n",
    "                'window_1': w1,\n",
    "                'window_2': w2,\n",
    "                'core_1_size': len(core1),\n",
    "                'core_2_size': len(core2),\n",
    "                'shared': shared,\n",
    "                'only_in_w1': only_w1,\n",
    "                'only_in_w2': only_w2,\n",
    "                'jaccard': jaccard\n",
    "            })\n",
    "            \n",
    "            print(f\"    {w1} -> {w2}: Jaccard={jaccard:.3f}, shared={shared}, lost={only_w1}, gained={only_w2}\")\n",
    "\n",
    "jaccard_df = pd.DataFrame(jaccard_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualize fixed window cores\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, species in enumerate(SPECIES):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    species_data = window_df[window_df['species'] == species]\n",
    "    windows = species_data['window'].unique()\n",
    "    \n",
    "    x = np.arange(len(windows))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, threshold in enumerate(THRESHOLDS):\n",
    "        thresh_data = species_data[species_data['threshold'] == threshold]\n",
    "        thresh_data = thresh_data.set_index('window').loc[windows]\n",
    "        ax.bar(x + i*width, thresh_data['core_size'], width,\n",
    "               label=f\"{int(threshold*100)}%\", color=colors[threshold], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Time Window')\n",
    "    ax.set_ylabel('Core Size')\n",
    "    ax.set_title(f'{species}\\nCore Size by Time Window')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(windows, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/fixed_window_cores.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: fixed_window_cores.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "## Track Core Turnover: Which Genes Leave/Enter Core?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Identify stable core vs transient core genes\n",
    "\n",
    "# For each species, identify:\n",
    "# - Genes that are core in ALL windows (stable core)\n",
    "# - Genes that are core in SOME windows (transient)\n",
    "# - Genes that enter/leave core over time\n",
    "\n",
    "core_stability = []\n",
    "\n",
    "for species in SPECIES:\n",
    "    print(f\"\\n=== Core stability analysis: {species} ===\")\n",
    "    \n",
    "    windows = sorted(window_cores[species].keys())\n",
    "    \n",
    "    for threshold in THRESHOLDS:\n",
    "        # Get all cores\n",
    "        all_cores = [window_cores[species][w].get(threshold, set()) for w in windows]\n",
    "        all_cores = [c for c in all_cores if len(c) > 0]  # Filter empty\n",
    "        \n",
    "        if len(all_cores) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Stable core = intersection of all window cores\n",
    "        stable_core = set.intersection(*all_cores)\n",
    "        \n",
    "        # Ever-core = union of all window cores\n",
    "        ever_core = set.union(*all_cores)\n",
    "        \n",
    "        # Transient = ever-core minus stable\n",
    "        transient_core = ever_core - stable_core\n",
    "        \n",
    "        print(f\"  {int(threshold*100)}% threshold:\")\n",
    "        print(f\"    Stable core (in ALL windows): {len(stable_core)}\")\n",
    "        print(f\"    Transient (in SOME windows): {len(transient_core)}\")\n",
    "        print(f\"    Ever-core (union): {len(ever_core)}\")\n",
    "        \n",
    "        core_stability.append({\n",
    "            'species': species,\n",
    "            'threshold': threshold,\n",
    "            'n_windows': len(all_cores),\n",
    "            'stable_core_size': len(stable_core),\n",
    "            'transient_core_size': len(transient_core),\n",
    "            'ever_core_size': len(ever_core),\n",
    "            'stability_ratio': len(stable_core) / len(ever_core) if len(ever_core) > 0 else 0\n",
    "        })\n",
    "\n",
    "stability_df = pd.DataFrame(core_stability)\n",
    "print(\"\\nCore stability summary:\")\n",
    "print(stability_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Track when specific genes leave the core\n",
    "\n",
    "# For the cumulative analysis, identify at what point each gene leaves the core\n",
    "\n",
    "gene_exit_points = []\n",
    "\n",
    "for species in SPECIES:\n",
    "    print(f\"\\n=== Gene exit tracking: {species} ===\")\n",
    "    \n",
    "    ordered_genomes = data[species]['ordered_genomes']\n",
    "    genome_clusters = data[species]['genome_clusters']\n",
    "    genomes_df = data[species]['genomes_df']\n",
    "    \n",
    "    # Use 95% threshold\n",
    "    threshold = 0.95\n",
    "    \n",
    "    # Track at key points (every 100 genomes)\n",
    "    check_points = list(range(MIN_WINDOW_SIZE, len(ordered_genomes), 100))\n",
    "    if len(ordered_genomes) not in check_points:\n",
    "        check_points.append(len(ordered_genomes))\n",
    "    \n",
    "    # Calculate core at each checkpoint\n",
    "    checkpoint_cores = {}\n",
    "    for n in tqdm(check_points, desc=\"Calculating cores\"):\n",
    "        checkpoint_cores[n] = calculate_core(ordered_genomes[:n], genome_clusters, threshold)\n",
    "    \n",
    "    # For each gene in initial core, find when it exits\n",
    "    initial_core = checkpoint_cores[check_points[0]]\n",
    "    \n",
    "    for gene_id in initial_core:\n",
    "        exit_point = None\n",
    "        for n in check_points:\n",
    "            if gene_id not in checkpoint_cores[n]:\n",
    "                exit_point = n\n",
    "                break\n",
    "        \n",
    "        gene_exit_points.append({\n",
    "            'species': species,\n",
    "            'gene_cluster_id': gene_id,\n",
    "            'exit_at_n_genomes': exit_point,\n",
    "            'exits_early': exit_point is not None and exit_point < len(ordered_genomes) // 2,\n",
    "            'never_exits': exit_point is None\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    species_exits = [e for e in gene_exit_points if e['species'] == species]\n",
    "    never_exits = sum(1 for e in species_exits if e['never_exits'])\n",
    "    early_exits = sum(1 for e in species_exits if e['exits_early'])\n",
    "    print(f\"  Initial core: {len(initial_core)}\")\n",
    "    print(f\"  Never exit (stable): {never_exits}\")\n",
    "    print(f\"  Exit early (< 50%): {early_exits}\")\n",
    "\n",
    "exit_df = pd.DataFrame(gene_exit_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save all results\n",
    "\n",
    "# Cumulative results\n",
    "cumulative_df.to_parquet(f\"{OUTPUT_PATH}/cumulative_results.parquet\")\n",
    "print(f\"Saved: cumulative_results.parquet ({len(cumulative_df)} rows)\")\n",
    "\n",
    "# Window results\n",
    "window_df.to_parquet(f\"{OUTPUT_PATH}/window_results.parquet\")\n",
    "print(f\"Saved: window_results.parquet ({len(window_df)} rows)\")\n",
    "\n",
    "# Inter-window Jaccard\n",
    "jaccard_df.to_csv(f\"{OUTPUT_PATH}/inter_window_jaccard.csv\", index=False)\n",
    "print(f\"Saved: inter_window_jaccard.csv ({len(jaccard_df)} rows)\")\n",
    "\n",
    "# Core stability\n",
    "stability_df.to_csv(f\"{OUTPUT_PATH}/core_stability.csv\", index=False)\n",
    "print(f\"Saved: core_stability.csv ({len(stability_df)} rows)\")\n",
    "\n",
    "# Gene exit points\n",
    "exit_df.to_parquet(f\"{OUTPUT_PATH}/gene_exit_points.parquet\")\n",
    "print(f\"Saved: gene_exit_points.parquet ({len(exit_df)} rows)\")\n",
    "\n",
    "# Save stable core gene lists\n",
    "for species in SPECIES:\n",
    "    windows = sorted(window_cores[species].keys())\n",
    "    all_cores_95 = [window_cores[species][w].get(0.95, set()) for w in windows]\n",
    "    all_cores_95 = [c for c in all_cores_95 if len(c) > 0]\n",
    "    if len(all_cores_95) >= 2:\n",
    "        stable = set.intersection(*all_cores_95)\n",
    "        with open(f\"{OUTPUT_PATH}/{species}_stable_core_genes.txt\", 'w') as f:\n",
    "            for gene_id in sorted(stable):\n",
    "                f.write(f\"{gene_id}\\n\")\n",
    "        print(f\"Saved: {species}_stable_core_genes.txt ({len(stable)} genes)\")\n",
    "\n",
    "print(\"\\nAll results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Files Created\n",
    "- `cumulative_results.parquet` - Core size at each cumulative step\n",
    "- `window_results.parquet` - Core size for each fixed time window\n",
    "- `inter_window_jaccard.csv` - Jaccard similarity between adjacent windows\n",
    "- `core_stability.csv` - Stable vs transient core statistics\n",
    "- `gene_exit_points.parquet` - When each gene exits the core\n",
    "- `{species}_stable_core_genes.txt` - Gene IDs in stable core\n",
    "- `cumulative_core_decay.png` - Visualization of core decay\n",
    "- `fixed_window_cores.png` - Core size by time window\n",
    "\n",
    "### Next Steps\n",
    "Run `03_analysis.ipynb` for:\n",
    "- Decay rate fitting (power law)\n",
    "- Species comparison\n",
    "- Detailed turnover analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
