{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Temporal Core Genome Dynamics - Data Extraction\n",
    "\n",
    "## Goal\n",
    "Extract genome data with collection dates and gene cluster memberships for:\n",
    "- **Pseudomonas aeruginosa** (~6,760 genomes)\n",
    "- **Acinetobacter baumannii** (~6,647 genomes)\n",
    "\n",
    "## Data Needed\n",
    "1. Genomes with valid collection dates (from ncbi_env table)\n",
    "2. Gene cluster memberships per genome (which clusters present in which genomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, count, countDistinct\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_PATH = \"/home/psdehal/pangenome_science/data/temporal_core\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Target species (using LIKE pattern to catch the species prefix)\n",
    "SPECIES = {\n",
    "    'p_aeruginosa': 's__Pseudomonas_aeruginosa',\n",
    "    'a_baumannii': 's__Acinetobacter_baumannii'\n",
    "}\n",
    "\n",
    "print(f\"Output path: {OUTPUT_PATH}\")\n",
    "print(f\"Target species: {list(SPECIES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: Extract Genomes with Collection Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Query genomes with collection dates for both species\n",
    "\n",
    "# Get genomes with collection date metadata\n",
    "# Filter out invalid date values\n",
    "\n",
    "INVALID_DATES = [\n",
    "    'missing', 'not applicable', 'not collected', \n",
    "    'Unknown', 'unknown', 'not provided', 'NA', 'N/A'\n",
    "]\n",
    "\n",
    "genomes_with_dates = {}\n",
    "\n",
    "for species_key, species_prefix in SPECIES.items():\n",
    "    print(f\"\\n=== Extracting {species_key} ===\")\n",
    "    \n",
    "    # Query genomes with collection dates\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            g.genome_id,\n",
    "            g.gtdb_species_clade_id,\n",
    "            g.ncbi_biosample_id,\n",
    "            ne.content as collection_date_raw\n",
    "        FROM kbase_ke_pangenome.genome g\n",
    "        JOIN kbase_ke_pangenome.ncbi_env ne\n",
    "            ON g.ncbi_biosample_id = ne.accession\n",
    "        WHERE g.gtdb_species_clade_id LIKE '{species_prefix}%'\n",
    "            AND ne.attribute_name = 'collection_date'\n",
    "            AND ne.content IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query).toPandas()\n",
    "    print(f\"  Total genomes with collection_date field: {len(df)}\")\n",
    "    \n",
    "    # Filter out invalid dates\n",
    "    df_valid = df[~df['collection_date_raw'].isin(INVALID_DATES)].copy()\n",
    "    print(f\"  After removing invalid values: {len(df_valid)}\")\n",
    "    \n",
    "    genomes_with_dates[species_key] = df_valid\n",
    "    \n",
    "    # Show sample dates\n",
    "    print(f\"  Sample dates: {df_valid['collection_date_raw'].head(10).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Parse collection dates into standardized format\n",
    "\n",
    "def parse_collection_date(date_str):\n",
    "    \"\"\"\n",
    "    Parse variable date formats from ncbi_env collection_date field.\n",
    "    \n",
    "    Common formats:\n",
    "    - YYYY-MM-DD (2015-03-12)\n",
    "    - YYYY/MM/DD (2015/03/12)\n",
    "    - YYYY-MM (2015-03)\n",
    "    - YYYY (2015)\n",
    "    - Date ranges (2013/2014, 2015-04/2016-09)\n",
    "    \n",
    "    Returns: datetime object or None\n",
    "    For partial dates (YYYY or YYYY-MM), uses mid-point (July 1 or 15th)\n",
    "    For ranges, uses start date\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str) or not isinstance(date_str, str):\n",
    "        return None\n",
    "    \n",
    "    date_str = date_str.strip()\n",
    "    \n",
    "    # Handle ranges - take first date\n",
    "    if '/' in date_str and len(date_str) > 10:\n",
    "        date_str = date_str.split('/')[0]\n",
    "    \n",
    "    # Try various formats\n",
    "    formats_to_try = [\n",
    "        ('%Y-%m-%d', None),       # 2015-03-12 -> exact\n",
    "        ('%Y/%m/%d', None),       # 2015/03/12 -> exact\n",
    "        ('%Y-%m', 'month'),       # 2015-03 -> mid-month (15th)\n",
    "        ('%Y/%m', 'month'),       # 2015/03 -> mid-month\n",
    "        ('%Y', 'year'),           # 2015 -> mid-year (July 1)\n",
    "    ]\n",
    "    \n",
    "    for fmt, granularity in formats_to_try:\n",
    "        try:\n",
    "            dt = datetime.strptime(date_str, fmt)\n",
    "            if granularity == 'month':\n",
    "                dt = dt.replace(day=15)\n",
    "            elif granularity == 'year':\n",
    "                dt = dt.replace(month=7, day=1)\n",
    "            return dt\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Try to extract just the year\n",
    "    year_match = re.search(r'(19|20)\\d{2}', date_str)\n",
    "    if year_match:\n",
    "        try:\n",
    "            year = int(year_match.group())\n",
    "            return datetime(year, 7, 1)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Apply parsing to both species\n",
    "for species_key, df in genomes_with_dates.items():\n",
    "    print(f\"\\n=== Parsing dates for {species_key} ===\")\n",
    "    \n",
    "    df['collection_date'] = df['collection_date_raw'].apply(parse_collection_date)\n",
    "    \n",
    "    n_parsed = df['collection_date'].notna().sum()\n",
    "    n_total = len(df)\n",
    "    print(f\"  Successfully parsed: {n_parsed}/{n_total} ({100*n_parsed/n_total:.1f}%)\")\n",
    "    \n",
    "    # Filter to only parsed dates\n",
    "    df_parsed = df[df['collection_date'].notna()].copy()\n",
    "    genomes_with_dates[species_key] = df_parsed\n",
    "    \n",
    "    # Date range\n",
    "    print(f\"  Date range: {df_parsed['collection_date'].min()} to {df_parsed['collection_date'].max()}\")\n",
    "    \n",
    "    # Show failed parses\n",
    "    failed = df[df['collection_date'].isna()]['collection_date_raw'].unique()[:10]\n",
    "    print(f\"  Sample unparsed values: {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Visualize date distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (species_key, df) in enumerate(genomes_with_dates.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Extract years\n",
    "    years = df['collection_date'].dt.year\n",
    "    \n",
    "    ax.hist(years, bins=range(years.min(), years.max()+2), \n",
    "            edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Collection Year')\n",
    "    ax.set_ylabel('Number of Genomes')\n",
    "    ax.set_title(f\"{species_key}\\n(n={len(df)} genomes with dates)\")\n",
    "    ax.axvline(years.median(), color='red', linestyle='--', \n",
    "               label=f'Median: {int(years.median())}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/collection_date_distributions.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: collection_date_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Save genome lists with dates\n",
    "\n",
    "for species_key, df in genomes_with_dates.items():\n",
    "    output_file = f\"{OUTPUT_PATH}/{species_key}_genomes.parquet\"\n",
    "    \n",
    "    # Prepare for export\n",
    "    export_df = df[['genome_id', 'gtdb_species_clade_id', \n",
    "                    'collection_date_raw', 'collection_date']].copy()\n",
    "    export_df['collection_year'] = export_df['collection_date'].dt.year\n",
    "    \n",
    "    # Sort by collection date\n",
    "    export_df = export_df.sort_values('collection_date').reset_index(drop=True)\n",
    "    \n",
    "    # Save\n",
    "    export_df.to_parquet(output_file)\n",
    "    print(f\"Saved {len(export_df)} genomes to {output_file}\")\n",
    "    \n",
    "    # Summary stats\n",
    "    print(f\"  Year range: {export_df['collection_year'].min()} - {export_df['collection_year'].max()}\")\n",
    "    print(f\"  Genomes per year: {export_df.groupby('collection_year').size().describe()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: Extract Gene Cluster Memberships\n",
    "\n",
    "For each genome, we need to know which gene clusters are present.\n",
    "This creates a genome-to-cluster presence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Extract gene cluster memberships per genome\n",
    "\n",
    "# For each species, get which gene clusters are present in which genomes\n",
    "# This joins: gene -> gene_genecluster_junction -> gene_cluster\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "for species_key, species_prefix in SPECIES.items():\n",
    "    print(f\"\\n=== Extracting gene clusters for {species_key} ===\")\n",
    "    \n",
    "    # Get target genome IDs (those with valid dates)\n",
    "    target_genomes = genomes_with_dates[species_key]['genome_id'].tolist()\n",
    "    print(f\"  Target genomes: {len(target_genomes)}\")\n",
    "    \n",
    "    # Query gene clusters present in each genome\n",
    "    # Aggregate to get unique (genome_id, gene_cluster_id) pairs\n",
    "    query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            g.genome_id,\n",
    "            gg.gene_cluster_id\n",
    "        FROM kbase_ke_pangenome.gene g\n",
    "        JOIN kbase_ke_pangenome.gene_genecluster_junction gg\n",
    "            ON g.gene_id = gg.gene_id\n",
    "        WHERE g.genome_id IN ({','.join([f\"'{gid}'\" for gid in target_genomes])})\n",
    "    \"\"\"\n",
    "    \n",
    "    # This could be large - count first\n",
    "    clusters_df = spark.sql(query)\n",
    "    clusters_df.cache()\n",
    "    total_pairs = clusters_df.count()\n",
    "    print(f\"  Total genome-cluster pairs: {total_pairs:,}\")\n",
    "    \n",
    "    # Get cluster counts\n",
    "    n_clusters = clusters_df.select('gene_cluster_id').distinct().count()\n",
    "    print(f\"  Unique gene clusters: {n_clusters:,}\")\n",
    "    \n",
    "    # Export in chunks if large\n",
    "    CLUSTERS_PATH = f\"{OUTPUT_PATH}/{species_key}_gene_clusters\"\n",
    "    os.makedirs(CLUSTERS_PATH, exist_ok=True)\n",
    "    \n",
    "    CHUNK_SIZE = 5000000\n",
    "    if total_pairs > CHUNK_SIZE:\n",
    "        clusters_with_id = clusters_df.withColumn(\"_id\", monotonically_increasing_id())\n",
    "        n_chunks = (total_pairs // CHUNK_SIZE) + 1\n",
    "        \n",
    "        for i in range(n_chunks):\n",
    "            start_id = i * CHUNK_SIZE\n",
    "            end_id = (i + 1) * CHUNK_SIZE\n",
    "            \n",
    "            chunk = clusters_with_id.filter(\n",
    "                (col(\"_id\") >= start_id) & (col(\"_id\") < end_id)\n",
    "            ).drop(\"_id\")\n",
    "            \n",
    "            chunk_pd = chunk.toPandas()\n",
    "            chunk_pd.to_parquet(f\"{CLUSTERS_PATH}/chunk_{i:03d}.parquet\")\n",
    "            print(f\"    Saved chunk {i+1}/{n_chunks}: {len(chunk_pd):,} rows\")\n",
    "    else:\n",
    "        # Small enough to save as single file\n",
    "        clusters_pd = clusters_df.toPandas()\n",
    "        clusters_pd.to_parquet(f\"{CLUSTERS_PATH}/all_clusters.parquet\")\n",
    "        print(f\"  Saved all {len(clusters_pd):,} pairs\")\n",
    "    \n",
    "    clusters_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3: Verify Data and Create Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Verification and summary\n",
    "\n",
    "print(\"=== DATA EXTRACTION SUMMARY ===\")\n",
    "print()\n",
    "\n",
    "summary_stats = []\n",
    "\n",
    "for species_key in SPECIES.keys():\n",
    "    # Load genome data\n",
    "    genomes_df = pd.read_parquet(f\"{OUTPUT_PATH}/{species_key}_genomes.parquet\")\n",
    "    \n",
    "    # Load cluster data\n",
    "    clusters_path = f\"{OUTPUT_PATH}/{species_key}_gene_clusters\"\n",
    "    if os.path.exists(f\"{clusters_path}/all_clusters.parquet\"):\n",
    "        clusters_df = pd.read_parquet(f\"{clusters_path}/all_clusters.parquet\")\n",
    "    else:\n",
    "        # Combine chunks\n",
    "        chunks = []\n",
    "        for f in sorted(os.listdir(clusters_path)):\n",
    "            if f.endswith('.parquet'):\n",
    "                chunks.append(pd.read_parquet(f\"{clusters_path}/{f}\"))\n",
    "        clusters_df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Statistics\n",
    "    n_genomes = len(genomes_df)\n",
    "    n_clusters = clusters_df['gene_cluster_id'].nunique()\n",
    "    year_min = genomes_df['collection_year'].min()\n",
    "    year_max = genomes_df['collection_year'].max()\n",
    "    genes_per_genome = clusters_df.groupby('genome_id').size()\n",
    "    \n",
    "    print(f\"{species_key}:\")\n",
    "    print(f\"  Genomes with dates: {n_genomes:,}\")\n",
    "    print(f\"  Unique gene clusters: {n_clusters:,}\")\n",
    "    print(f\"  Collection years: {year_min} - {year_max}\")\n",
    "    print(f\"  Clusters per genome: {genes_per_genome.mean():.0f} mean, {genes_per_genome.min()}-{genes_per_genome.max()} range\")\n",
    "    print()\n",
    "    \n",
    "    summary_stats.append({\n",
    "        'species': species_key,\n",
    "        'n_genomes': n_genomes,\n",
    "        'n_clusters': n_clusters,\n",
    "        'year_min': year_min,\n",
    "        'year_max': year_max,\n",
    "        'mean_clusters_per_genome': genes_per_genome.mean()\n",
    "    })\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df.to_csv(f\"{OUTPUT_PATH}/extraction_summary.csv\", index=False)\n",
    "print(f\"Summary saved to: {OUTPUT_PATH}/extraction_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Compare with full pangenome statistics\n",
    "\n",
    "# Get expected core sizes from pangenome table for validation\n",
    "for species_key, species_prefix in SPECIES.items():\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            p.gtdb_species_clade_id,\n",
    "            p.no_genomes,\n",
    "            p.no_core,\n",
    "            p.no_aux_genome,\n",
    "            p.no_singleton_gene_clusters,\n",
    "            s.mean_intra_species_ANI\n",
    "        FROM kbase_ke_pangenome.pangenome p\n",
    "        JOIN kbase_ke_pangenome.gtdb_species_clade s\n",
    "            ON p.gtdb_species_clade_id = s.gtdb_species_clade_id\n",
    "        WHERE p.gtdb_species_clade_id LIKE '{species_prefix}%'\n",
    "    \"\"\"\n",
    "    \n",
    "    result = spark.sql(query).toPandas()\n",
    "    \n",
    "    print(f\"\\n{species_key} - Full pangenome statistics:\")\n",
    "    print(result.to_string(index=False))\n",
    "    print(f\"\\n  Our dated subset: {len(genomes_with_dates[species_key])} genomes\")\n",
    "    print(f\"  Coverage: {100 * len(genomes_with_dates[species_key]) / result['no_genomes'].iloc[0]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "After running this notebook, the following files are created:\n",
    "\n",
    "- `p_aeruginosa_genomes.parquet` - Genomes with parsed collection dates\n",
    "- `a_baumannii_genomes.parquet` - Genomes with parsed collection dates\n",
    "- `p_aeruginosa_gene_clusters/` - Gene cluster memberships (genome_id, gene_cluster_id)\n",
    "- `a_baumannii_gene_clusters/` - Gene cluster memberships\n",
    "- `collection_date_distributions.png` - Visualization of date distributions\n",
    "- `extraction_summary.csv` - Summary statistics\n",
    "\n",
    "**Next Step**: Run `02_sliding_window.ipynb` to calculate core genome across time windows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
