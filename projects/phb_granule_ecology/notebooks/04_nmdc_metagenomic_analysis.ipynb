{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NB04: NMDC Metagenomic Analysis of PHB Pathway Prevalence\n",
    "\n",
    "**Purpose**: Test PHB pathway prevalence across NMDC environments using independent metagenomic data.\n",
    "\n",
    "**Requires**: BERDL JupyterHub (Spark session)\n",
    "\n",
    "**Inputs**:\n",
    "- `data/phb_species_summary.tsv` from NB01\n",
    "- `data/phb_by_taxonomy.tsv` from NB02\n",
    "\n",
    "**Outputs**:\n",
    "- `data/nmdc_phb_prevalence.tsv` — per-sample PHB inference scores\n",
    "- `data/nmdc_study_summary.tsv` — study-level PHB summary\n",
    "- `figures/nmdc_phb_by_environment.png` — PHB prevalence across NMDC environments\n",
    "- `figures/nmdc_phb_vs_abiotic.png` — PHB signal vs abiotic variability\n",
    "\n",
    "## Strategy\n",
    "\n",
    "Two paths depending on data availability (explored in NB01):\n",
    "- **Path A** (preferred): If per-sample functional annotations (KO counts) exist, directly count PHB pathway KOs\n",
    "- **Path B** (fallback): Infer PHB capability from taxonomic composition × pangenome PHB status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "PROJECT_DIR = os.path.expanduser('~/BERIL-research-observatory/projects/phb_granule_ecology')\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "FIG_DIR = os.path.join(PROJECT_DIR, 'figures')\n",
    "\n",
    "# Load NB01/NB02 results\n",
    "species_phb = pd.read_csv(os.path.join(DATA_DIR, 'phb_species_summary.tsv'), sep='\\t')\n",
    "tax_phb = pd.read_csv(os.path.join(DATA_DIR, 'phb_by_taxonomy.tsv'), sep='\\t')\n",
    "print(f'Loaded {len(species_phb):,} PHB+ species, {len(tax_phb):,} total species')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: NMDC Study & Sample Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of NMDC studies\n",
    "studies = spark.sql(\"\"\"\n",
    "    SELECT * FROM nmdc_arkin.study_table\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'Total NMDC studies: {len(studies)}')\n",
    "print(f'\\nColumns: {list(studies.columns)}')\n",
    "studies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available abiotic features — these provide environmental context\n",
    "abiotic = spark.sql(\"\"\"\n",
    "    SELECT * FROM nmdc_arkin.abiotic_features LIMIT 5\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'Abiotic feature columns ({len(abiotic.columns)}):') \n",
    "for col in abiotic.columns:\n",
    "    print(f'  {col}')\n",
    "abiotic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples have abiotic data?\n",
    "abiotic_count = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as n_samples FROM nmdc_arkin.abiotic_features\n",
    "\"\"\").toPandas()\n",
    "print(f'Samples with abiotic features: {abiotic_count[\"n_samples\"].iloc[0]:,}')\n",
    "\n",
    "# Check taxonomy features — this is our primary path for PHB inference\n",
    "tax_feat_sample = spark.sql(\"\"\"\n",
    "    SELECT * FROM nmdc_arkin.taxonomy_features LIMIT 5\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'\\nTaxonomy feature columns ({len(tax_feat_sample.columns)}):')\n",
    "# Show first 20 columns and last 5\n",
    "cols = list(tax_feat_sample.columns)\n",
    "for c in cols[:20]:\n",
    "    print(f'  {c}')\n",
    "if len(cols) > 25:\n",
    "    print(f'  ... ({len(cols) - 25} more columns) ...')\n",
    "    for c in cols[-5:]:\n",
    "        print(f'  {c}')\n",
    "\n",
    "tax_feat_count = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as n FROM nmdc_arkin.taxonomy_features\n",
    "\"\"\").toPandas()\n",
    "print(f'\\nSamples with taxonomy features: {tax_feat_count[\"n\"].iloc[0]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for per-sample functional annotations (Path A)\n",
    "# Look for tables with KO counts per sample\n",
    "print('=== Checking for per-sample functional annotation tables ===')\n",
    "\n",
    "# Check metatranscriptomics_gold — might have per-sample functional counts\n",
    "try:\n",
    "    mt_sample = spark.sql(\"SELECT * FROM nmdc_arkin.metatranscriptomics_gold LIMIT 3\").toPandas()\n",
    "    print(f'\\nmetatranscriptomics_gold columns: {list(mt_sample.columns)}')\n",
    "    print(mt_sample)\n",
    "except Exception as e:\n",
    "    print(f'metatranscriptomics_gold: {e}')\n",
    "\n",
    "# Check trait_features — has functional group columns\n",
    "try:\n",
    "    trait_sample = spark.sql(\"SELECT * FROM nmdc_arkin.trait_features LIMIT 3\").toPandas()\n",
    "    func_cols = [c for c in trait_sample.columns if 'functional_group' in c.lower() or 'function' in c.lower()]\n",
    "    print(f'\\ntrait_features: {len(trait_sample.columns)} columns')\n",
    "    print(f'Functional group columns: {len(func_cols)}')\n",
    "    for c in func_cols[:15]:\n",
    "        print(f'  {c}: {trait_sample[c].iloc[0]}')\n",
    "except Exception as e:\n",
    "    print(f'trait_features: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if KEGG KO annotations exist per sample\n",
    "# annotation_terms_unified has COG, EC, GO, KEGG, MetaCyc — but is it per-sample?\n",
    "print('=== Checking annotation_terms_unified ===')\n",
    "ann_sample = spark.sql(\"\"\"\n",
    "    SELECT * FROM nmdc_arkin.annotation_terms_unified \n",
    "    WHERE source = 'KEGG'\n",
    "    LIMIT 5\n",
    "\"\"\").toPandas()\n",
    "print(f'Columns: {list(ann_sample.columns)}')\n",
    "ann_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Check if PHB-related KOs exist in NMDC KEGG terms\nphb_kos = ['K03821', 'K00023', 'K00626', 'K05973', 'K14205', 'K18080']\nko_str = \"','\".join(phb_kos)\n\nphb_in_nmdc = spark.sql(f\"\"\"\n    SELECT * FROM nmdc_arkin.kegg_ko_terms\n    WHERE ko_id IN ('{ko_str}')\n\"\"\").toPandas()\n\nprint('PHB KOs in NMDC KEGG reference:')\nif len(phb_in_nmdc) > 0:\n    print(phb_in_nmdc[['ko_id', 'name']].to_string(index=False))\nelse:\n    # Inspect table schema if no results\n    all_kegg = spark.sql(\"SELECT * FROM nmdc_arkin.kegg_ko_terms LIMIT 5\").toPandas()\n    print(f'Columns: {list(all_kegg.columns)}')\n    print(all_kegg)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Part 2: Taxonomy-Based PHB Inference (Path B)\n",
    "\n",
    "The `taxonomy_features` table contains per-sample taxonomic profiles from metagenomic classification.\n",
    "We infer PHB capability for each sample by weighting the taxonomic composition by the pangenome PHB status from NB01/NB02.\n",
    "\n",
    "**PHB inference score** = Σ (relative_abundance_taxon_i × phaC_prevalence_in_clade_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Load taxonomy dimension table to map numeric column IDs to taxon names\n# taxonomy_features columns are numeric IDs (7, 11, 33, ...), not taxon names\ntax_dim = spark.sql(\"\"\"\n    SELECT * FROM nmdc_arkin.taxonomy_dim\n\"\"\").toPandas()\n\nprint(f'taxonomy_dim: {tax_dim.shape[0]} rows x {tax_dim.shape[1]} columns')\nprint(f'Columns: {list(tax_dim.columns)}')\ntax_dim.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Also check taxstring_lookup for alternative ID-to-name mapping\ntry:\n    taxstring = spark.sql(\"\"\"\n        SELECT * FROM nmdc_arkin.taxstring_lookup LIMIT 20\n    \"\"\").toPandas()\n    print(f'taxstring_lookup columns: {list(taxstring.columns)}')\n    print(f'Shape: {taxstring.shape}')\n    taxstring.head(10)\nexcept Exception as e:\n    print(f'taxstring_lookup: {e}')\n\n# Load taxonomy features\ntax_features = spark.sql(\"\"\"\n    SELECT * FROM nmdc_arkin.taxonomy_features\n\"\"\").toPandas()\n\nprint(f'\\nTaxonomy features: {tax_features.shape[0]} samples x {tax_features.shape[1]} columns')\ntaxon_cols = [c for c in tax_features.columns if c != 'sample_id']\nprint(f'Taxon columns (first 20): {taxon_cols[:20]}')\nprint(f'Total taxon columns: {len(taxon_cols)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Build mapping from numeric taxon IDs (column names in taxonomy_features)\n# to genus names that we can match to pangenome data\n#\n# Strategy: taxonomy_dim should have columns like taxon_id + taxon name/rank info\n# We need to extract the genus-level name for each taxon ID\n\n# Identify the ID column and name/rank columns in taxonomy_dim\nprint('taxonomy_dim columns and sample values:')\nfor col in tax_dim.columns:\n    print(f'  {col}: {tax_dim[col].iloc[:3].tolist()}')\n\n# Build ID → genus mapping\n# The exact approach depends on taxonomy_dim schema (will adapt after seeing output above)\n# Common patterns: taxon_id + lineage string, or taxon_id + rank + name\n\n# Try to find genus from taxonomy_dim\nid_col = tax_dim.columns[0]  # First column is likely the ID\nprint(f'\\nUsing \"{id_col}\" as taxonomy ID column')\n\n# Check which taxonomy_features column IDs exist in taxonomy_dim\ntaxon_id_set = set(str(x) for x in tax_dim[id_col])\nmatched_ids = [c for c in taxon_cols if c in taxon_id_set]\nprint(f'Taxon columns matched to taxonomy_dim: {len(matched_ids)}/{len(taxon_cols)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Extract genus from taxonomy_dim and build ID → genus lookup\n# Adapt based on what columns are available in taxonomy_dim\n\n# Look for columns containing lineage, genus, name, rank\nname_candidates = [c for c in tax_dim.columns \n                   if any(kw in c.lower() for kw in ['genus', 'name', 'lineage', 'tax', 'string', 'rank'])]\nprint(f'Potential name/lineage columns: {name_candidates}')\n\n# Build the taxon_id → genus name mapping\n# Will use the first suitable column that contains genus-level information\ntaxid_to_genus = {}\n\nfor col in name_candidates:\n    sample_vals = tax_dim[col].dropna().head(5).tolist()\n    print(f'\\n  {col} samples: {sample_vals}')\n\n# Extract genus from lineage string (e.g., \"d__Bacteria;p__Proteobacteria;...;g__Pseudomonas;s__...\")\n# or from a dedicated genus column\nfor col in tax_dim.columns:\n    vals = tax_dim[col].dropna().astype(str)\n    # Check if this column contains genus-level taxonomy strings\n    has_genus_prefix = vals.str.contains('g__', na=False).any()\n    if has_genus_prefix:\n        print(f'\\nFound genus-level info in column: {col}')\n        # Parse genus from taxonomy string\n        for _, row in tax_dim.iterrows():\n            taxid = str(row[id_col])\n            lineage = str(row[col])\n            # Extract genus from lineage (format: ...;g__GenusName;...)\n            parts = lineage.split(';')\n            genus = None\n            for p in parts:\n                p = p.strip()\n                if p.startswith('g__') and len(p) > 3:\n                    genus = p.replace('g__', '').strip()\n                    break\n            if genus:\n                taxid_to_genus[taxid] = genus\n        break\n\nprint(f'\\nMapped {len(taxid_to_genus)} taxon IDs to genus names')\nif taxid_to_genus:\n    sample_items = list(taxid_to_genus.items())[:5]\n    for tid, genus in sample_items:\n        print(f'  ID {tid} → {genus}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Build genus-level PHB prevalence from pangenome data\ngenus_phb = tax_phb.groupby('gtdb_genus').agg(\n    n_species=('gtdb_species_clade_id', 'count'),\n    n_phaC=('has_phaC', 'sum'),\n    pct_phaC=('has_phaC', lambda x: x.mean()),  # proportion, not percentage\n).reset_index()\ngenus_phb['genus_clean'] = genus_phb['gtdb_genus'].str.replace('g__', '', regex=False).str.strip()\n\n# Match NMDC taxon IDs → genus → pangenome PHB prevalence\ngenus_phb_lookup = dict(zip(genus_phb['genus_clean'].str.lower(), genus_phb['pct_phaC']))\n\n# Build matched list: (taxon_col_name, genus_name) for columns we can map\nmatched = []\nunmatched_ids = []\nfor col_id in taxon_cols:\n    genus = taxid_to_genus.get(col_id, None)\n    if genus and genus.lower() in genus_phb_lookup:\n        matched.append((col_id, genus.lower()))\n    else:\n        unmatched_ids.append(col_id)\n\nprint(f'Matched taxon IDs to pangenome genera: {len(matched)}/{len(taxon_cols)}')\nprint(f'Unmatched: {len(unmatched_ids)}')\nif matched:\n    print(f'\\nFirst 10 matches:')\n    for col_id, genus in matched[:10]:\n        print(f'  ID {col_id} → {genus} (phaC: {genus_phb_lookup[genus]*100:.1f}%)')\n\n# Compute per-sample PHB inference score\nsample_scores = []\nfor _, row in tax_features.iterrows():\n    sample_id = row['sample_id']\n    phb_score = 0.0\n    total_abundance = 0.0\n    matched_abundance = 0.0\n    \n    for col_id, genus in matched:\n        abundance = pd.to_numeric(row.get(col_id, 0), errors='coerce')\n        if pd.notna(abundance) and abundance > 0:\n            phb_score += abundance * genus_phb_lookup.get(genus, 0)\n            matched_abundance += abundance\n    \n    for col in taxon_cols:\n        val = pd.to_numeric(row.get(col, 0), errors='coerce')\n        if pd.notna(val) and val > 0:\n            total_abundance += val\n    \n    sample_scores.append({\n        'sample_id': sample_id,\n        'phb_score': phb_score,\n        'matched_abundance': matched_abundance,\n        'total_abundance': total_abundance,\n        'pct_matched': matched_abundance / total_abundance * 100 if total_abundance > 0 else 0,\n    })\n\nsample_phb = pd.DataFrame(sample_scores)\nprint(f'\\nComputed PHB scores for {len(sample_phb):,} samples')\nprint(f'\\nPHB score distribution:')\nprint(sample_phb['phb_score'].describe())\nprint(f'\\nTaxon matching coverage:')\nprint(f'  Median % abundance matched: {sample_phb[\"pct_matched\"].median():.1f}%')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Part 3: Correlate PHB Signal with Abiotic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load abiotic features\n",
    "abiotic_all = spark.sql(\"\"\"\n",
    "    SELECT * FROM nmdc_arkin.abiotic_features\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'Abiotic features: {abiotic_all.shape[0]} samples x {abiotic_all.shape[1]} columns')\n",
    "\n",
    "# Merge with PHB scores\n",
    "phb_abiotic = sample_phb.merge(abiotic_all, on='sample_id', how='inner')\n",
    "print(f'Samples with both PHB scores and abiotic data: {len(phb_abiotic):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key abiotic variables for feast/famine hypothesis\n",
    "# Focus on: dissolved oxygen, pH, temperature, carbon-related measures\n",
    "abiotic_cols = [c for c in abiotic_all.columns if c != 'sample_id']\n",
    "\n",
    "# Cast numeric columns and check coverage\n",
    "print('Abiotic variable coverage:')\n",
    "abiotic_coverage = []\n",
    "for col in abiotic_cols:\n",
    "    vals = pd.to_numeric(phb_abiotic[col], errors='coerce')\n",
    "    n_valid = vals.notna().sum()\n",
    "    if n_valid > 0:\n",
    "        abiotic_coverage.append({\n",
    "            'column': col,\n",
    "            'n_valid': n_valid,\n",
    "            'pct_valid': n_valid / len(phb_abiotic) * 100,\n",
    "            'mean': vals.mean(),\n",
    "            'std': vals.std(),\n",
    "        })\n",
    "\n",
    "abiotic_cov_df = pd.DataFrame(abiotic_coverage).sort_values('n_valid', ascending=False)\n",
    "print(abiotic_cov_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link samples to studies for within-study variability analysis\n",
    "# Check if study_table has sample-to-study mapping\n",
    "print('study_table columns:', list(studies.columns))\n",
    "\n",
    "# Try to get sample-study links from embeddings or metadata\n",
    "try:\n",
    "    emb_meta = spark.sql(\"\"\"\n",
    "        SELECT * FROM nmdc_arkin.embedding_metadata LIMIT 5\n",
    "    \"\"\").toPandas()\n",
    "    print(f'\\nembedding_metadata columns: {list(emb_meta.columns)}')\n",
    "    emb_meta\n",
    "except Exception as e:\n",
    "    print(f'embedding_metadata: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate PHB score with key abiotic variables\n",
    "# Select columns with sufficient coverage\n",
    "target_keywords = ['oxygen', 'ph', 'temp', 'carbon', 'nitro', 'ammonium', 'salinity']\n",
    "key_abiotic = []\n",
    "for _, row in abiotic_cov_df.iterrows():\n",
    "    col = row['column']\n",
    "    if row['n_valid'] >= 30:  # minimum for meaningful correlation\n",
    "        key_abiotic.append(col)\n",
    "\n",
    "print(f'Abiotic variables with >=30 valid values: {len(key_abiotic)}')\n",
    "\n",
    "# Compute Spearman correlations with PHB score\n",
    "correlations = []\n",
    "for col in key_abiotic:\n",
    "    vals = pd.to_numeric(phb_abiotic[col], errors='coerce')\n",
    "    valid = vals.notna() & phb_abiotic['phb_score'].notna()\n",
    "    if valid.sum() >= 30:\n",
    "        rho, p = stats.spearmanr(phb_abiotic.loc[valid, 'phb_score'], vals[valid])\n",
    "        correlations.append({\n",
    "            'abiotic_variable': col,\n",
    "            'n': valid.sum(),\n",
    "            'spearman_rho': rho,\n",
    "            'p_value': p,\n",
    "        })\n",
    "\n",
    "corr_df = pd.DataFrame(correlations).sort_values('p_value')\n",
    "print('\\nSpearman correlations: PHB score vs abiotic variables')\n",
    "print(corr_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check trait_features for PHB-relevant functional groups\n",
    "traits = spark.sql(\"\"\"\n",
    "    SELECT * FROM nmdc_arkin.trait_features\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'Trait features: {traits.shape[0]} samples x {traits.shape[1]} columns')\n",
    "\n",
    "# Look for PHB-related traits\n",
    "phb_trait_cols = [c for c in traits.columns \n",
    "                  if any(kw in c.lower() for kw in ['pha', 'phb', 'polyhydrox', \n",
    "                         'carbon_storage', 'granule', 'storage'])]\n",
    "print(f'\\nPHB-related trait columns: {phb_trait_cols}')\n",
    "\n",
    "# Also check for general metabolic traits that might correlate\n",
    "metab_trait_cols = [c for c in traits.columns \n",
    "                    if any(kw in c.lower() for kw in ['ferment', 'aerob', 'anaerob',\n",
    "                           'respir', 'nitrogen', 'carbon', 'fatty_acid'])]\n",
    "print(f'Metabolic trait columns: {metab_trait_cols[:15]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If PHB-related traits exist, correlate with our PHB inference score\n",
    "if phb_trait_cols:\n",
    "    traits_phb = traits[['sample_id'] + phb_trait_cols].merge(\n",
    "        sample_phb[['sample_id', 'phb_score']], on='sample_id', how='inner')\n",
    "    \n",
    "    for col in phb_trait_cols:\n",
    "        vals = pd.to_numeric(traits_phb[col], errors='coerce')\n",
    "        valid = vals.notna() & traits_phb['phb_score'].notna()\n",
    "        if valid.sum() >= 10:\n",
    "            rho, p = stats.spearmanr(traits_phb.loc[valid, 'phb_score'], vals[valid])\n",
    "            print(f'{col}: rho={rho:.3f}, p={p:.2e}, n={valid.sum()}')\n",
    "else:\n",
    "    print('No PHB-specific trait columns found in trait_features.')\n",
    "    print('Will rely on taxonomy-based inference only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check metabolomics for 3-hydroxybutyrate / PHB monomers\n",
    "try:\n",
    "    metab_sample = spark.sql(\"\"\"\n",
    "        SELECT * FROM nmdc_arkin.metabolomics_gold LIMIT 5\n",
    "    \"\"\").toPandas()\n",
    "    print(f'metabolomics_gold columns: {list(metab_sample.columns)}')\n",
    "    metab_sample\n",
    "except Exception as e:\n",
    "    print(f'metabolomics_gold error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# Search metabolomics for 3-hydroxybutyrate or related compounds\n# From NB01b: columns include \"Compound Name\", \"Common Name\", \"Traditional Name\", \"name\"\n# Use backticks for columns with spaces\ntry:\n    phb_metabolites = spark.sql(\"\"\"\n        SELECT DISTINCT `Compound Name`, `Common Name`, `Traditional Name`, name, kegg, smiles,\n               `Sample Name`\n        FROM nmdc_arkin.metabolomics_gold\n        WHERE LOWER(`Compound Name`) LIKE '%hydroxybutyrat%'\n           OR LOWER(`Compound Name`) LIKE '%phb%'\n           OR LOWER(name) LIKE '%hydroxybutyrat%'\n           OR LOWER(`Common Name`) LIKE '%hydroxybutyrat%'\n        LIMIT 20\n    \"\"\").toPandas()\n    \n    if len(phb_metabolites) > 0:\n        print(f'Found PHB-related metabolites: {len(phb_metabolites)}')\n        print(phb_metabolites)\n    else:\n        print('No PHB-related metabolites found by name.')\nexcept Exception as e:\n    print(f'Metabolomics query failed: {e}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Part 4: Visualize PHB Signal Across NMDC Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify NMDC samples by environment type using study metadata and abiotic features\n",
    "# Merge PHB scores with available metadata\n",
    "\n",
    "# Attempt to get environment labels from embedding_metadata or study_table\n",
    "try:\n",
    "    emb_meta_all = spark.sql(\"\"\"\n",
    "        SELECT * FROM nmdc_arkin.embedding_metadata\n",
    "    \"\"\").toPandas()\n",
    "    print(f'Embedding metadata: {len(emb_meta_all)} samples')\n",
    "    print(f'Columns: {list(emb_meta_all.columns)}')\n",
    "    \n",
    "    # Check for environment-related columns\n",
    "    env_cols = [c for c in emb_meta_all.columns if any(kw in c.lower() \n",
    "                for kw in ['env', 'ecosystem', 'habitat', 'biome', 'study'])]\n",
    "    print(f'\\nEnvironment-related columns: {env_cols}')\n",
    "    if env_cols:\n",
    "        for col in env_cols:\n",
    "            print(f'\\n{col} value counts:')\n",
    "            print(emb_meta_all[col].value_counts().head(10))\n",
    "except Exception as e:\n",
    "    print(f'embedding_metadata: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: PHB inference score distribution across samples\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(sample_phb['phb_score'], bins=50, color='#2196F3', alpha=0.8, edgecolor='white')\n",
    "ax.set_xlabel('PHB inference score')\n",
    "ax.set_ylabel('Number of samples')\n",
    "ax.set_title('Distribution of PHB Scores Across NMDC Samples')\n",
    "ax.axvline(sample_phb['phb_score'].median(), color='red', linestyle='--', \n",
    "           label=f'Median={sample_phb[\"phb_score\"].median():.3f}')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.hist(sample_phb['pct_matched'], bins=50, color='#4CAF50', alpha=0.8, edgecolor='white')\n",
    "ax.set_xlabel('% taxonomic abundance matched to pangenome')\n",
    "ax.set_ylabel('Number of samples')\n",
    "ax.set_title('Pangenome Matching Coverage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'nmdc_phb_by_environment.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: PHB score vs key abiotic variables\n",
    "# Plot top correlations (by p-value)\n",
    "if len(corr_df) > 0:\n",
    "    top_n = min(4, len(corr_df))\n",
    "    top_corr = corr_df.head(top_n)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, top_n, figsize=(4*top_n, 4))\n",
    "    if top_n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_corr.iterrows()):\n",
    "        col = row['abiotic_variable']\n",
    "        ax = axes[i]\n",
    "        vals = pd.to_numeric(phb_abiotic[col], errors='coerce')\n",
    "        valid = vals.notna() & phb_abiotic['phb_score'].notna()\n",
    "        \n",
    "        ax.scatter(vals[valid], phb_abiotic.loc[valid, 'phb_score'], \n",
    "                   alpha=0.3, s=10, color='#2196F3')\n",
    "        # Clean column name for display\n",
    "        clean_name = col.replace('annotations_', '').replace('_has_numeric_value', '')\n",
    "        ax.set_xlabel(clean_name)\n",
    "        ax.set_ylabel('PHB score')\n",
    "        ax.set_title(f'rho={row[\"spearman_rho\"]:.2f}, p={row[\"p_value\"]:.1e}')\n",
    "    \n",
    "    plt.suptitle('PHB Score vs Abiotic Variables (Top Correlations)', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIG_DIR, 'nmdc_phb_vs_abiotic.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No significant abiotic correlations found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "sample_phb.to_csv(os.path.join(DATA_DIR, 'nmdc_phb_prevalence.tsv'), sep='\\t', index=False)\n",
    "\n",
    "# Save correlation results\n",
    "if len(corr_df) > 0:\n",
    "    corr_df.to_csv(os.path.join(DATA_DIR, 'nmdc_abiotic_correlations.tsv'), sep='\\t', index=False)\n",
    "\n",
    "print(f'Saved PHB prevalence: {len(sample_phb):,} samples')\n",
    "print(f'Saved abiotic correlations: {len(corr_df)} variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings (to be filled after execution)\n",
    "- Per-sample functional annotations available?: ?\n",
    "- Taxonomy-based PHB inference: ? samples scored\n",
    "- Pangenome matching coverage: ?%\n",
    "- Top abiotic correlations: ?\n",
    "- PHB-related metabolites found: ?\n",
    "\n",
    "### Caveats\n",
    "- Taxonomy-based inference is indirect — assumes genus-level PHB prevalence applies to metagenome taxa\n",
    "- Taxonomic classification methods (Centrifuge/Kraken/GOTTCHA) have different genus-level accuracy\n",
    "- NMDC samples are biased toward specific ecosystems (soil, aquatic) per NMDC study portfolio\n",
    "- Abiotic features are snapshots, not measures of temporal variability\n",
    "\n",
    "### Next Notebook (NB05)\n",
    "Subclade enrichment and cross-validation — test for differential PHB enrichment within clades and validate pangenome patterns against NMDC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}